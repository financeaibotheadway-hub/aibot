from google.cloud import bigquery
from vertexai.preview.generative_models import GenerativeModel
import json
from semantic_map import semantic_map
import pandas as pd
import re
import hashlib
import time
from functools import lru_cache

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤
bq_client = bigquery.Client()
model = GenerativeModel("gemini-2.5-flash")

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–∞–±–ª–∏—Ü—ñ
project_id = "thermal-beach-465608-h7"
dataset_id = "test_vertex_ai"
table_id = "Revenue"
table_ref = f"{project_id}.{dataset_id}.{table_id}"

# –ö–µ—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑–∞–ø–∏—Ç—ñ–≤
query_cache = {}
cache_ttl = 300  # 5 —Ö–≤–∏–ª–∏–Ω

# –ö–µ—à —Å—Ö–µ–º–∏
_schema_cache = None
_schema_time = 0


def get_cache_key(query: str) -> str:
    return hashlib.md5(query.encode()).hexdigest()


# –ö–µ—à–æ–≤–∞–Ω–∞ —Å—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü—ñ
def get_table_schema():
    global _schema_cache, _schema_time
    current_time = time.time()

    if _schema_cache is None or current_time - _schema_time > 3600:  # 1 –≥–æ–¥–∏–Ω–∞
        schema = bq_client.get_table(table_ref).schema
        _schema_cache = [{"name": f.name, "type": f.field_type} for f in schema]
        _schema_time = current_time

    return _schema_cache


schema_info = get_table_schema()


# –ö–µ—à–æ–≤–∞–Ω–∏–π –∑–∞–ø–∏—Ç
def execute_cached_query(sql_query):
    cache_key = get_cache_key(sql_query)
    current_time = time.time()

    if cache_key in query_cache:
        cached_data, timestamp = query_cache[cache_key]
        if current_time - timestamp < cache_ttl:
            return cached_data
        else:
            del query_cache[cache_key]

    query_job = bq_client.query(sql_query)
    df = query_job.result().to_dataframe()

    query_cache[cache_key] = (df.copy(), current_time)

    # –û–±–º–µ–∂—É—î–º–æ –∫–µ—à
    if len(query_cache) > 20:
        oldest_key = min(query_cache.keys(), key=lambda k: query_cache[k][1])
        del query_cache[oldest_key]

    return df


# –í–∞–ª—ñ–¥–∞—Ü—ñ—è SQL –ø–µ—Ä–µ–¥ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º
def validate_sql_syntax(sql_query):
    """–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏ BigQuery"""
    errors = []

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ window functions –∑ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–º ORDER BY
    window_pattern = r'(?:ROW_NUMBER|RANK|DENSE_RANK|LAG|LEAD)\s*\(\s*\)\s+OVER\s*\([^)]*ORDER\s+BY\s+([^)]+)\)'
    window_matches = re.findall(window_pattern, sql_query, re.IGNORECASE)

    for order_expr in window_matches:
        # –Ø–∫—â–æ ORDER BY –º—ñ—Å—Ç–∏—Ç—å –ø–æ–ª–µ, —â–æ –Ω–µ –∑–≥—Ä—É–ø–æ–≤–∞–Ω–µ
        if 'GROUP BY' in sql_query.upper() and not any(
                field in sql_query.split('GROUP BY')[1] for field in order_expr.split(',')):
            errors.append(f"Window ORDER BY –º—ñ—Å—Ç–∏—Ç—å –ø–æ–ª–µ '{order_expr.strip()}', —è–∫–µ –Ω–µ –∑–≥—Ä—É–ø–æ–≤–∞–Ω–µ")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ correlated subqueries
    if re.search(r'WHERE\s+\w+\s+IN\s*\(\s*SELECT.*WHERE.*\w+\.\w+\s*=\s*\w+\.\w+', sql_query,
                 re.IGNORECASE | re.DOTALL):
        errors.append("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –ø—ñ–¥–∑–∞–ø–∏—Ç–∏, —è–∫—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è BigQuery")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è STRFTIME
    if 'STRFTIME' in sql_query.upper():
        errors.append("STRFTIME –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –≤ BigQuery. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ FORMAT_DATE")

    return errors


# AI –∑–∞–º—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó
@lru_cache(maxsize=100)
def find_matches_with_ai_cached(instruction, semantic_map_str):
    semantic_map = json.loads(semantic_map_str)

    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ semantic_map –≤ –∑—Ä–æ–∑—É–º—ñ–ª–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è AI
    context = {}
    for full_key, phrases in semantic_map.items():
        field, value = full_key.split(":")
        if field not in context:
            context[field] = {}

        synonyms = []
        for p in phrases:
            if isinstance(p, dict):
                synonyms.append(p.get("text", ""))
            else:
                synonyms.append(str(p))
        context[field][value] = synonyms

    prompt = f"""
–ó–Ω–∞–π–¥–∏ —è–∫—ñ –ø–æ–ª—è –∑–≥–∞–¥—É—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Å–∏–Ω–æ–Ω—ñ–º–∏:

–î–æ—Å—Ç—É–ø–Ω—ñ –ø–æ–ª—è —Ç–∞ —Å–∏–Ω–æ–Ω—ñ–º–∏:
{json.dumps(context, ensure_ascii=False, indent=2)}

–¢–µ–∫—Å—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: "{instruction}"

–ü—Ä–∞–≤–∏–ª–∞:
- –Ø–∫—â–æ "—Ñ—ñ" + "—Ä–µ—Ñ–∞–Ω–¥" ‚Üí event_type=refund_fee
- –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ "—Ä–µ—Ñ–∞–Ω–¥" ‚Üí event_type=refund  
- –Ø–∫—â–æ "—Ñ—ñ" + "—á–∞—Ä–¥–∂–±–µ–∫" ‚Üí event_type=chargeback_fee
- –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ "—á–∞—Ä–¥–∂–±–µ–∫" ‚Üí event_type=chargeback

"""

    try:
        response = model.generate_content(prompt, generation_config={"temperature": 0})
        result = response.text.strip()

        if result == "NONE":
            return []

        matches = []
        for pair in result.split(','):
            if ':' in pair:
                field, value = pair.strip().split(':', 1)
                matches.append((field, value))
        return matches
    except:
        return []


def find_matches_with_ai(instruction, semantic_map):
    semantic_map_str = json.dumps(semantic_map, sort_keys=True)
    return find_matches_with_ai_cached(instruction, semantic_map_str)


# –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –æ–∫—Ä–µ–º—ñ –∑–∞–ø–∏—Ç–∏
def split_into_separate_queries(message: str) -> list:
    """–†–æ–∑–¥—ñ–ª—è—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –Ω–∞ –æ–∫—Ä–µ–º—ñ –∑–∞–ø–∏—Ç–∏"""

    split_prompt = f"""
–†–æ–∑–¥—ñ–ª–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–∞ –æ–∫—Ä–µ–º—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –∑–∞–ø–∏—Ç–∏. –ö–æ–∂–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –∑–∞–≤–¥–∞–Ω–Ω—è –º–∞—î –±—É—Ç–∏ –æ–∫—Ä–µ–º–∏–º –∑–∞–ø–∏—Ç–æ–º.

–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: "{message}"

–ó–Ω–∞–π–¥–∏ –≤—Å—ñ –æ–∫—Ä–µ–º—ñ –ø–∏—Ç–∞–Ω–Ω—è/–∑–∞–≤–¥–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–ª—ñ—á–∏ —ó—Ö –≤ —Ç–∞–∫–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ:
–ó–ê–ü–ò–¢_1: [–ø–µ—Ä—à–∏–π –∑–∞–ø–∏—Ç]
–ó–ê–ü–ò–¢_2: [–¥—Ä—É–≥–∏–π –∑–∞–ø–∏—Ç]
–ó–ê–ü–ò–¢_3: [—Ç—Ä–µ—Ç—ñ–π –∑–∞–ø–∏—Ç]
... —ñ —Ç–∞–∫ –¥–∞–ª—ñ

–ü—Ä–∞–≤–∏–ª–∞:
- –Ø–∫—â–æ –≤ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—ñ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è/–∑–∞–≤–¥–∞–Ω–Ω—è, –ø–æ–≤–µ—Ä–Ω–∏ —Ç—ñ–ª—å–∫–∏ –ó–ê–ü–ò–¢_1
- –ö–æ–∂–Ω–∏–π –∑–∞–ø–∏—Ç –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –ø–æ–≤–Ω–∏–º —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º —Å–∞–º –ø–æ —Å–æ–±—ñ
- –ù–µ –¥–æ–¥–∞–≤–∞–π –∑–∞–ø–∏—Ç–∏, —è–∫–∏—Ö –Ω–µ–º–∞—î –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—ñ
- –ó–±–µ—Ä—ñ–≥–∞–π –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Å–µ–Ω—Å –∫–æ–∂–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É

–ü—Ä–∏–∫–ª–∞–¥–∏:
"–°–∫—ñ–ª—å–∫–∏ –±—É–ª–æ –ø—Ä–æ–¥–∞–∂—ñ–≤ —É —Å—ñ—á–Ω—ñ? –ê —Å–∫—ñ–ª—å–∫–∏ —Ä–µ—Ñ–∞–Ω–¥—ñ–≤?" ‚Üí
–ó–ê–ü–ò–¢_1: –°–∫—ñ–ª—å–∫–∏ –±—É–ª–æ –ø—Ä–æ–¥–∞–∂—ñ–≤ —É —Å—ñ—á–Ω—ñ?
–ó–ê–ü–ò–¢_2: –°–∫—ñ–ª—å–∫–∏ –±—É–ª–æ —Ä–µ—Ñ–∞–Ω–¥—ñ–≤ —É —Å—ñ—á–Ω—ñ?

"–ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–æ–ø –∫—Ä–∞—ó–Ω –∑–∞ –¥–æ—Ö–æ–¥–æ–º" ‚Üí
–ó–ê–ü–ò–¢_1: –ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–æ–ø –∫—Ä–∞—ó–Ω –∑–∞ –¥–æ—Ö–æ–¥–æ–º
"""

    try:
        response = model.generate_content(split_prompt, generation_config={"temperature": 0})
        result = response.text.strip()

        # –ü–∞—Ä—Å–∏–º–æ –∑–∞–ø–∏—Ç–∏
        queries = []
        lines = result.split('\n')

        for line in lines:
            line = line.strip()
            if line.startswith('–ó–ê–ü–ò–¢_'):
                parts = line.split(':', 1)
                if len(parts) == 2:
                    query = parts[1].strip()
                    if query:
                        queries.append(query)

        return queries if queries else [message]

    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—ñ –Ω–∞ –∑–∞–ø–∏—Ç–∏: {e}")
        return [message]


# –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –í–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø–∏—Ç—É
def execute_single_query(instruction: str, semantic_map: dict) -> str:
    """–í–∏–∫–æ–Ω—É—î –æ–¥–∏–Ω –ø—Ä–æ—Å—Ç–∏–π –∑–∞–ø–∏—Ç"""

    try:
        instruction_part = instruction.strip()
        if not instruction_part:
            return "–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—î. –ù–∞–ø–∏—à–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é."

        # AI –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è –∑–∞–º—ñ—Å—Ç—å —Å–∫–ª–∞–¥–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É
        matched_conditions = find_matches_with_ai(instruction_part, semantic_map)

        for field, value in matched_conditions:
            instruction_part += f" ({field} = '{value}')"

        sql_prompt = f"""
–°—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü—ñ:
{json.dumps(schema_info, indent=2)}

–ó–≥–µ–Ω–µ—Ä—É–π –ï–ö–°–ü–ï–†–¢–ù–ò–ô BigQuery SQL-–∑–∞–ø–∏—Ç –¥–ª—è: {instruction_part}

–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞:
- –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ç—ñ–ª—å–∫–∏ BigQuery SQL;
- –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π STRFTIME, –∑–∞–º—ñ—Å—Ç—å —Ü—å–æ–≥–æ FORMAT_DATE('%Y-%m', DATE(...));
- –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –ø—ñ–¥–∑–∞–ø–∏—Ç–∏ (correlated subqueries);
- –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ window functions, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —ó—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑ GROUP BY;
- –Ω–∞–∑–≤–∏ —Ç–∞–±–ª–∏—Ü—å —Ç–∞ –ø–æ–ª—ñ–≤ –±–µ—Ä–∏ –∑—ñ —Å—Ö–µ–º–∏;
- –Ω–∞–∑–≤—É —Ç–∞–±–ª–∏—Ü—ñ –≤–∫–∞–∂–∏ —è–∫ {table_ref};
- —è–∫—â–æ –≤ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –ø—Ä–∏—Å—É—Ç–Ω—ñ–π —Ç–µ–∫—Å—Ç '(field = "...")', –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ü–µ —è–∫ —á–∞—Å—Ç–∏–Ω—É WHERE;
- –ø–æ–≤–µ—Ä–Ω–∏ —Ç—ñ–ª—å–∫–∏ SQL-–∑–∞–ø–∏—Ç;
- "–æ–ø–∏—Å" = "description";
- –∫–æ–ª–∏ –∫–∞–∂—É—Ç—å "net revenue" —á–∏ "–Ω–µ—Ç —Ä–µ–≤–µ–Ω—å—é" –≤ —Ä–µ—á–µ–Ω–Ω—ñ(–Ω–µ—Ç —ñ —Ä–µ–≤–µ–Ω—å—é –º–æ–∂—É—Ç—å –±—É—Ç–∏ –Ω–µ –ø–æ—Ä—É—á –æ–¥–Ω–µ –∑ –æ–¥–Ω–∏–º), —Å—É–º—É–π –≤–µ—Å—å —Å—Ç–æ–≤–ø–µ—Ü—å gross_usd —ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π event_type = "sale", –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —É—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —É —Å—Ç–æ–≤–ø—Ü—ñ event_type;
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π revenue_type = 'New' —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó —è–≤–Ω–æ –∑–≥–∞–¥–∞–Ω–æ —Å–ª–æ–≤–æ "–Ω–æ–≤–∏–π" –∞–±–æ "–Ω—å—é". –Ø–∫—â–æ –π–¥–µ—Ç—å—Å—è –ø—Ä–æ –¥–æ—Ö—ñ–¥ —á–∏ –ø—Ä–æ —Ä–µ–≤–µ–Ω—å—é –∑–∞–≥–∞–ª–æ–º, –Ω–µ –¥–æ–¥–∞–≤–∞–π —Ü–µ–π —Ñ—ñ–ª—å—Ç—Ä.

–°–¢–†–ê–¢–ï–ì–Ü–Ø –ê–ù–ê–õ–Ü–ó–£:
–ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ—è—Å–Ω–∏—Ç–∏ –∑–º—ñ–Ω–∏ –≤ –¥–∞–Ω–∏—Ö –∞–±–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è - –∑–∞–≤–∂–¥–∏ –∞–Ω–∞–ª—ñ–∑—É–π app_name, event_type, revenue_type, period (—è–∫ –æ–∫—Ä–µ–º–æ, —Ç–∞–∫ —ñ –≤ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è—Ö). –°–∞–º–µ –≤–æ–Ω–∏ –¥–∞—é—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –ø—Ä–∏—á–∏–Ω–∏ –ø–æ–¥—ñ–π.

–ï–∫—Å–ø–µ—Ä—Ç–Ω—ñ —Ç–µ—Ö–Ω—ñ–∫–∏:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π CTEs (WITH) –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
- APPROX_QUANTILES –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç—ñ–ª—ñ–≤
- SAFE_DIVIDE –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –¥—ñ–ª–µ–Ω–Ω—è
- ROW_NUMBER() OVER –¥–ª—è —Ä–∞–Ω–≥—É–≤–∞–Ω–Ω—è
- LAG/LEAD –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω—å –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –ø–µ—Ä—ñ–æ–¥–∞–º–∏


üö® –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û - –ü–û–õ–ï period:
- period –º—ñ—Å—Ç–∏—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É: 12M, 1M, 6M, 3M —Ç–æ—â–æ
- –¶–ï –ù–ï –ß–ê–°–û–í–Ü –ü–ï–†–Ü–û–î–ò! –¶–µ –∫–æ–¥–∏ —Ç–∏–ø—ñ–≤ –ø—ñ–¥–ø–∏—Å–∫–∏!
- 12M = —Ç–∏–ø –ø—ñ–¥–ø–∏—Å–∫–∏ "12-–º—ñ—Å—è—á–Ω–∞", 1M = —Ç–∏–ø –ø—ñ–¥–ø–∏—Å–∫–∏ "–º—ñ—Å—è—á–Ω–∞"
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π period –¢–Ü–õ–¨–ö–ò –¥–ª—è GROUP BY –ø–æ —Ç–∏–ø–∞—Ö –ø—ñ–¥–ø–∏—Å–∫–∏
- –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π period –≤ ORDER BY –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ç—Ä–µ–Ω–¥—ñ–≤
- –ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π period –≤ LAG/LEAD —Ñ—É–Ω–∫—Ü—ñ—è—Ö
        """

        response = model.generate_content(sql_prompt, generation_config={"temperature": 0})
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.lower().startswith("sql"):
            sql_query = sql_query[3:].strip()

        # –®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ SQL –ø–µ—Ä–µ–¥ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º
        validation_errors = validate_sql_syntax(sql_query)
        if validation_errors:
            error_msg = "‚ùå **–ü–æ–º–∏–ª–∫–∞ –≤ –∑–∞–ø–∏—Ç—ñ:**\n"
            for error in validation_errors:
                error_msg += f"‚Ä¢ {error}\n"
            return error_msg

        try:
            df = execute_cached_query(sql_query)
        except Exception as bq_error:
            error_msg = "‚ùå **–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ –∑–∞–ø–∏—Ç—É –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.**\n"

            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–æ—Ä–∞–¥–∏ –¥–ª—è —Ç–∏–ø–æ–≤–∏—Ö –ø–æ–º–∏–ª–æ–∫
            if "Window ORDER BY" in str(bq_error):
                error_msg += "üí° **–ü–æ—Ä–∞–¥–∞:** –ü—Ä–æ–±–ª–µ–º–∞ –∑ window —Ñ—É–Ω–∫—Ü—ñ—î—é. –ü–µ—Ä–µ—Ñ—Ä–∞–∑—É–π—Ç–µ –∑–∞–ø–∏—Ç –ø—Ä–æ—Å—Ç—ñ—à–µ."
            elif "Correlated subqueries" in str(bq_error):
                error_msg += "üí° **–ü–æ—Ä–∞–¥–∞:** –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç –±–µ–∑ —Å–∫–ª–∞–¥–Ω–∏—Ö –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤."
            elif "invalidQuery" in str(bq_error):
                error_msg += "üí° **–ü–æ—Ä–∞–¥–∞:** –°–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤ SQL. –°–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ—Å—Ç—ñ—à–∏–π –∑–∞–ø–∏—Ç."

            return error_msg

        if df.empty:
            return "–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–±–ª–∏—Ü—ñ –ø–æ—Ä–æ–∂–Ω—ñ–π."

        analysis_prompt = f"""
–ó—Ä–æ–±–∏ —Ç–µ, —â–æ –ø—Ä–æ—Å–∏—Ç—å —Ç–µ–±–µ –∑—Ä–æ–±–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó.
–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:
"{instruction_part}"
CSV-—Ç–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É SQL-–∑–∞–ø–∏—Ç—É:

{df.to_csv(index=False)}

–û–±–æ–≤'—è–∑–∫–æ–≤–æ: 
- –ù–µ –ø–æ–≤–µ—Ä—Ç–∞–π –¥–∞–Ω—ñ —É —Ñ–æ—Ä–º–∞—Ç—ñ SQL.
- –ù–µ –ø–∏—à–∏ –≤—Å—Ç—É–ø '–∑ CSV —Ç–∞–±–ª–∏—Ü—ñ...".
- –ù–µ –≤–∏–≥–∞–¥—É–π –¥–∞–Ω—ñ –∞–±–æ –¥–∞—Ç–∏, —è–∫–∏—Ö –Ω–µ–º–∞—î –≤ —Ç–∞–±–ª–∏—Ü—ñ.
- –î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å —Ç–æ–≥–æ, —â–æ —Ç–∏ —à—É–∫–∞–≤, –Ω–µ –ø—Ä–∏–¥—É–º–∞–π —Ç–æ–≥–æ, —á–æ–≥–æ –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ –Ω–µ –±—É–ª–æ –≤ –ø—Ä–æ–º–ø—Ç—ñ —á–∏ –≤ –¥–∞–Ω–∏—Ö.
- –Ø–∫—â–æ —è –ø—Ä–æ—à—É –∑—Ä–æ–±–∏—Ç–∏ –∞–Ω–∞–ª—ñ—Ç–∏–∫—É –∞–±–æ –ø–æ—è—Å–Ω–∏—Ç–∏ –ø—Ä–∏—á–∏–Ω–∏ —Ç–∞ –Ω–∞—Å–ª—ñ–¥–∫–∏, —Å–∫–æ—Ä–æ—á—É–π –ø–æ—è—Å–Ω—é–≤–∞–ª—å–Ω—É –∞–±–æ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—É —á–∞—Å—Ç–∏–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–æ –º–∞–∫—Å–∏–º—É–º 3‚Äì4 —Ä–µ—á–µ–Ω—å.
- period (12M, 1M, 6M) - —Ü–µ —Ç–∏–ø–∏ –ø—ñ–¥–ø–∏—Å–æ–∫, –ù–ï —á–∞—Å–æ–≤—ñ –ø–µ—Ä—ñ–æ–¥–∏
- –ü—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ period –ø–æ–∫–∞–∑—É–π: "–ø—ñ–¥–ø–∏—Å–∫–∞ 12M –ø—Ä–∞—Ü—é—î –∫—Ä–∞—â–µ –Ω—ñ–∂ 1M"
- –ù–ï –≥–æ–≤–æ—Ä–∏ –ø—Ä–æ "—Ç—Ä–µ–Ω–¥–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö" - –≥–æ–≤–æ—Ä–∏ –ø—Ä–æ "–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–∏–ø—ñ–≤ –ø—ñ–¥–ø–∏—Å–æ–∫"
        """

        analysis_response = model.generate_content(analysis_prompt, generation_config={"temperature": 0})
        return analysis_response.text.strip()

    except Exception as e:
        return f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏:\n{str(e)}"


# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø: –û–±—Ä–æ–±–∫–∞ Slack-–ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –Ω–∞ –∑–∞–ø–∏—Ç–∏
def process_slack_message(message: str, semantic_map: dict) -> str:
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ–±—Ä–æ–±–∫–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""

    try:
        if not message.strip():
            return "–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—î. –ù–∞–ø–∏—à–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é."

        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞ –æ–∫—Ä–µ–º—ñ –∑–∞–ø–∏—Ç–∏
        queries = split_into_separate_queries(message)

        # –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω –∑–∞–ø–∏—Ç - –≤–∏–∫–æ–Ω—É—î–º–æ —è–∫ –∑–∞–∑–≤–∏—á–∞–π
        if len(queries) == 1:
            return execute_single_query(queries[0], semantic_map)

        # –Ø–∫—â–æ –∫—ñ–ª—å–∫–∞ –∑–∞–ø–∏—Ç—ñ–≤ - –≤–∏–∫–æ–Ω—É—î–º–æ –∫–æ–∂–µ–Ω –æ–∫—Ä–µ–º–æ
        results = []

        for i, query in enumerate(queries, 1):
            print(f"–í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É {i}/{len(queries)}: {query}")

            result = execute_single_query(query, semantic_map)
            results.append((i, query, result))

        # –§–æ—Ä–º—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        final_response = f"üìù **–ó–Ω–∞–π–¥–µ–Ω–æ {len(queries)} –∑–∞–ø–∏—Ç—ñ–≤. –í—ñ–¥–ø–æ–≤—ñ–¥–∞—é –Ω–∞ –∫–æ–∂–µ–Ω:**\n\n"

        for i, query, result in results:
            final_response += f"**üîç –ó–∞–ø–∏—Ç {i}:** *{query}*\n\n{result}\n\n"
            final_response += "=" * 60 + "\n\n"

        return final_response.rstrip("\n=").rstrip()

    except Exception as e:
        return f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:\n{str(e)}"


# –î–û–î–ê–¢–ö–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏—Å–Ω–æ–≤–∫—É
def generate_final_conclusion(results: list, original_message: str) -> str:
    """–ì–µ–Ω–µ—Ä—É—î –∑–∞–≥–∞–ª—å–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—Å—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""

    try:
        # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –≤–∏—Å–Ω–æ–≤–∫–∏ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        conclusions = []
        for i, query, result in results:
            if "–í–∏—Å–Ω–æ–≤–æ–∫:" in result:
                conclusion = result.split("–í–∏—Å–Ω–æ–≤–æ–∫:")[-1].strip()
                conclusions.append(f"–ó–∞–ø–∏—Ç {i}: {conclusion}")

        if not conclusions:
            return ""

        summary_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤—Å—ñ—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–∞–π –æ–¥–∏–Ω –∑–∞–≥–∞–ª—å–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫.

–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: "{original_message}"

–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞–ø–∏—Ç—ñ–≤:
{chr(10).join(conclusions)}

–°—Ç–≤–æ—Ä–∏ –æ–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–∞–ª—å–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ (2-4 —Ä–µ—á–µ–Ω–Ω—è), —è–∫–∏–π –ø—ñ–¥—Å—É–º–æ–≤—É—î –≤—Å—ñ –æ—Ç—Ä–∏–º–∞–Ω—ñ –¥–∞–Ω—ñ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:
"""

        response = model.generate_content(summary_prompt, generation_config={"temperature": 0})
        return f"üìã **–ó–ê–ì–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö:**\n{response.text.strip()}"

    except Exception as e:
        return f"üìã **–ó–ê–ì–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö:**\n–í—Å—ñ –∑–∞–ø–∏—Ç–∏ –æ–±—Ä–æ–±–ª–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ."


# –î–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –∫–µ—à–µ–º
def clear_cache():
    global query_cache, _schema_cache
    query_cache.clear()
    _schema_cache = None
    find_matches_with_ai_cached.cache_clear()


def get_cache_stats():
    return {
        "query_cache_size": len(query_cache),
        "ai_cache_info": find_matches_with_ai_cached.cache_info()
    }