# analytics.py
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
import logging
from functools import lru_cache
import asyncio
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest

import vertexai
from vertexai.preview.generative_models import GenerativeModel

from semantic_map import semantic_map  # ÑĞºÑ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENV / LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BQ_PROJECT       = os.getenv("BIGQUERY_PROJECT", "finance-ai-bot-headway")
BQ_DATASET       = os.getenv("BQ_DATASET", "uploads")
BQ_REVENUE_TABLE = os.getenv("BQ_REVENUE_TABLE", "revenue_test_databot")
BQ_COST_TABLE    = os.getenv("BQ_COST_TABLE", "cost_test_databot")
VERTEX_LOCATION  = os.getenv("VERTEX_LOCATION", "europe-west1")
LOCAL_TZ         = os.getenv("LOCAL_TZ", "Europe/Kyiv")     # TZ Ğ´Ğ»Ñ Ğ´Ğ°Ñ‚

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO))
logger = logging.getLogger("ai-bot")

# ÑĞºÑ‰Ğ¾ TRUE â€” Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ñƒ Slack Ğ´Ğ¾Ğ´Ğ°Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ñ–Ğ·Ğ°Ğ½Ğ¸Ğ¹ SQL Ñ– Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸
RETURN_SQL_ON_ERROR = os.getenv("RETURN_SQL_ON_ERROR", "false").lower() == "true"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THREADPOOL (Ğ³Ñ–Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¸Ğ¹ async + threads)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_WORKERS = int(os.getenv("AI_BOT_WORKERS", "4"))
_executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)


async def _run_in_executor(func, *args, **kwargs):
    """Ğ£Ğ½Ñ–Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ° Ğ¾Ğ±Ğ³Ğ¾Ñ€Ñ‚ĞºĞ°: Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¾Ñ— sync-Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ— Ğ² threadpool."""
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(_executor, lambda: func(*args, **kwargs))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INIT CLIENTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REVENUE_TABLE_REF = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_REVENUE_TABLE}"
COST_TABLE_REF    = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_COST_TABLE}"

# BigQuery
bq_client = bigquery.Client(project=BQ_PROJECT)

# Vertex AI
try:
    vertexai.init(project=BQ_PROJECT, location=VERTEX_LOCATION)
except Exception:
    logger.warning("Vertex init failed; will rely on ambient creds", exc_info=True)
model = GenerativeModel("gemini-2.5-flash")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CACHE (SQL + schemas)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query_cache = {}  # key -> (df, ts)
cache_ttl = 300   # seconds

_schema_cache = {}  # table_ref -> [{"name": ..., "type": ...}]
_schema_time  = {}  # table_ref -> ts


def get_cache_key(query: str) -> str:
    return hashlib.md5(query.encode("utf-8")).hexdigest()


def get_table_schema(table_ref: str, ttl_sec: int = 3600):
    """Return cached schema for table."""
    now = time.time()
    if (
        table_ref not in _schema_cache
        or table_ref not in _schema_time
        or now - _schema_time[table_ref] > ttl_sec
    ):
        schema = bq_client.get_table(table_ref).schema
        _schema_cache[table_ref] = [{"name": f.name, "type": f.field_type} for f in schema]
        _schema_time[table_ref] = now
    return _schema_cache[table_ref]


def get_all_schemas():
    rev_schema = get_table_schema(REVENUE_TABLE_REF)
    try:
        cost_schema = get_table_schema(COST_TABLE_REF)
    except Exception:
        cost_schema = []
    return rev_schema, cost_schema


# Ğ¿Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ½ÑŒĞ¾ Ñ–Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹ (ĞºĞ¾Ñ€Ğ¸ÑĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑˆĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°)
_ = get_all_schemas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ£Ğ¢Ğ˜Ğ›Ğ†Ğ¢Ğ˜ Ğ”ĞĞ¢ / SQL / CSV
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _collect_date_columns(schema_list):
    """ĞŸĞ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ğ½Ñƒ Ğ¿Ğ¾Ğ»Ñ–Ğ², ÑĞºÑ– Ğ¼Ğ°ÑÑ‚ÑŒ DATE/DATETIME/TIMESTAMP (Ñ‰Ğ¾Ğ± Ñ—Ñ… Ğ½Ğµ Ğ¿Ğ°Ñ€ÑĞ¸Ğ»Ğ¸ ÑĞº STRING)."""
    return {
        f["name"]
        for f in schema_list
        if f.get("type") in ("DATE", "DATETIME", "TIMESTAMP")
    }


def _sanitize_sql_dates(sql_query: str, date_columns: set) -> str:
    """
    ĞŸĞ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ° SQL: Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ” PARSE_DATE(..., <date_col>) Ñ‚Ğ° SAFE.PARSE_DATE Ğ´Ğ»Ñ Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ñ… DATE-Ğ¿Ğ¾Ğ»Ñ–Ğ²,
    Ğ¿Ñ–Ğ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ” CURRENT_DATE('<tz>') ÑĞºÑ‰Ğ¾ Ğ±ĞµĞ· TZ.
    """
    original = sql_query
    upper = sql_query.upper()

    # Ğ¯ĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ°Ñ” Ğ½Ñ– CURRENT_DATE, Ğ½Ñ– PARSE_DATE â€” Ğ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ñ€Ğ¾Ğ±Ğ¸Ğ¼Ğ¾
    if "CURRENT_DATE" not in upper and "PARSE_DATE" not in upper and "SAFE.PARSE_DATE" not in upper:
        return sql_query

    # 1) CURRENT_DATE() / CURRENT_DATE  â†’ CURRENT_DATE('Europe/Kyiv')
    sql_query = re.sub(
        r"\bCURRENT_DATE\s*\(\s*\)",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )
    sql_query = re.sub(
        r"\bCURRENT_DATE\b(?!\s*\()",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )

    # 2) Ğ”Ğ»Ñ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ¾Ğ³Ğ¾ DATE-Ğ¿Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ PARSE_DATE('%Y-%m-%d', col) -> col
    for col in sorted(date_columns, key=len, reverse=True):
        # Ğ· Ñ–Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸-Ğ°Ğ»Ñ–Ğ°ÑĞ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ñƒ t.posting_date Ğ°Ğ±Ğ¾ `posting_date`
        pattern_plain = rf"PARSE_DATE\(\s*'[^']+'\s*,\s*(`?[\w\.]+`?)\s*\)"

        def _repl_plain(m):
            inner = m.group(1)
            inner_clean = inner.strip("`")
            if inner_clean.endswith(f".{col}") or inner_clean == col:
                return inner
            return m.group(0)

        sql_query = re.sub(pattern_plain, _repl_plain, sql_query, flags=re.IGNORECASE)

        # SAFE.PARSE_DATE(...) -> CAST(col AS DATE)
        pattern_safe = rf"SAFE\.PARSE_DATE\(\s*'[^']+'\s*,\s*(`?[\w\.]+`?)\s*\)"

        def _repl_safe(m):
            inner = m.group(1)
            inner_clean = inner.strip("`")
            if inner_clean.endswith(f".{col}") or inner_clean == col:
                return f"CAST({inner} AS DATE)"
            return m.group(0)

        sql_query = re.sub(pattern_safe, _repl_safe, sql_query, flags=re.IGNORECASE)

    if sql_query != original:
        logger.info("[sanitize] SQL was sanitized for date handling")

    return sql_query


def normalize_sql(sql: str) -> str:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ SQL Ğ´Ğ»Ñ ĞºĞµÑˆÑƒ (Ñ‰Ğ¾Ğ± Ğ´Ñ€Ñ–Ğ±Ğ½Ñ– Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ğ½Ğ¾ÑÑ‚Ñ– Ğ½Ğµ Ğ»Ğ°Ğ¼Ğ°Ğ»Ğ¸ cache)."""
    sql = sql.strip()
    sql = re.sub(r"\s+", " ", sql)
    sql = sql.lower()
    return sql


def limited_csv(df: pd.DataFrame, max_rows: int = 7) -> str:
    """Ğ”Ğ°Ñ”Ğ¼Ğ¾ Vertex Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ½ĞµĞ²ĞµĞ»Ğ¸ĞºĞ¸Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ."""
    if df.empty:
        return "EMPTY_RESULT"

    if len(df) <= max_rows:
        return df.to_csv(index=False)

    # head + tail
    half = max_rows // 2
    head = df.head(half)
    tail = df.tail(max_rows - half)

    txt = "HEAD:\n" + head.to_csv(index=False)
    txt += "\nTAIL:\n" + tail.to_csv(index=False)
    txt += f"\n[TRUNCATED] total_rows={len(df)}"
    return txt


def auto_fix_group_by(sql: str) -> str:
    """ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ñ„Ñ–ĞºÑ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ 'column X which is neither grouped nor aggregated'."""
    try:
        m = re.search(r"select(.*?)from", sql, re.IGNORECASE | re.DOTALL)
        if not m:
            return sql
        select_block = m.group(1)

        fields = []
        for part in select_block.split(","):
            clean = part.strip()
            if not clean:
                continue

            # Ñ–Ğ³Ğ½Ğ¾Ñ€ÑƒÑ”Ğ¼Ğ¾ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¸
            if re.search(r"(sum|count|min|max|avg)\s*\(", clean, re.IGNORECASE):
                continue

            # Ğ²Ğ¸Ğ´Ğ°Ğ»ÑÑ”Ğ¼Ğ¾ alias
            clean_no_alias = re.sub(r"\s+as\s+.*", "", clean, flags=re.IGNORECASE).strip()

            # Ñ–Ğ³Ğ½Ğ¾Ñ€ÑƒÑ”Ğ¼Ğ¾ Ğ»Ñ–Ñ‚ĞµÑ€Ğ°Ğ»Ğ¸
            if clean_no_alias.startswith("'") or clean_no_alias.startswith('"'):
                continue

            fields.append(clean_no_alias)

        gb = re.search(
            r"group\s+by(.*?)(order\s+by|limit|$)",
            sql,
            re.IGNORECASE | re.DOTALL,
        )
        if not gb:
            return sql

        group_by_raw = gb.group(1)
        group_cols = [x.strip() for x in group_by_raw.split(",") if x.strip()]

        missing = []
        for f in fields:
            base_f = f.split(".")[-1].lower()
            found = any(base_f == g.split(".")[-1].lower() for g in group_cols)
            if not found:
                missing.append(f)

        if not missing:
            return sql

        new_group_by = "GROUP BY " + ", ".join(group_cols + missing)

        fixed_sql = re.sub(
            r"group\s+by(.*?)(order\s+by|limit|$)",
            new_group_by + r" \2",
            sql,
            flags=re.IGNORECASE | re.DOTALL,
        )
        logger.info("[auto_fix_group_by] added to GROUP BY: %s", missing)
        return fixed_sql
    except Exception:
        logger.exception("[auto_fix_group_by] failed")
        return sql


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXTRA SQL FIXES (FORMAT_DATE + Ğ·Ğ°Ğ¹Ğ²Ñ– ĞºĞ¾Ğ¼Ğ¸)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_format_date(sql: str) -> str:
    """Ğ›Ñ–ĞºÑƒÑ” Ñ‚Ğ¸Ğ¿Ğ¾Ğ²Ñ– Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ Ğ· FORMAT_DATE."""

    # FORMAT_DATE('%Y-%m')  â†’ Ğ²Ğ²Ğ°Ğ¶Ğ°Ñ”Ğ¼Ğ¾, Ñ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ·Ğ° posting_date
    sql = re.sub(
        r"FORMAT_DATE\s*\(\s*'([^']+)'\s*\)",
        r"FORMAT_DATE('\1', posting_date)",
        sql,
        flags=re.IGNORECASE,
    )

    # FORMAT_DATE('%Y-%m', posting_date,) â†’ Ğ¿Ñ€Ğ¸Ğ±Ñ€Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ¹Ğ²Ñƒ ĞºĞ¾Ğ¼Ñƒ
    sql = re.sub(
        r"FORMAT_DATE\s*\(([^,]+),\s*([^\)]+),\s*\)",
        r"FORMAT_DATE(\1, \2)",
        sql,
        flags=re.IGNORECASE,
    )

    # FORMAT_DATE('%Y-%m' posting_date) â†’ Ğ²ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚Ğ¸ ĞºĞ¾Ğ¼Ñƒ
    sql = re.sub(
        r"FORMAT_DATE\s*\(\s*'([^']+)'\s+(\w+)\s*\)",
        r"FORMAT_DATE('\1', \2)",
        sql,
        flags=re.IGNORECASE,
    )

    # FORMAT_DATE(...) Ğ±ĞµĞ· alias'Ğ° â†’ Ğ´Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ AS month
    sql = re.sub(
        r"(FORMAT_DATE\s*\([^\)]+\))(?!\s+as|\s*,)",
        r"\1 AS month",
        sql,
        flags=re.IGNORECASE,
    )

    return sql


def fix_trailing_commas(sql: str) -> str:
    """ĞŸÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ” Ğ·Ğ°Ğ¹Ğ²Ñ– ĞºĞ¾Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ FROM/WHERE/GROUP BY/ORDER BY Ñ– Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°ĞºÑ€Ğ¸Ğ²Ğ°ÑÑ‡Ğ¾Ñ Ğ´ÑƒĞ¶ĞºĞ¾Ñ."""
    # ĞºĞ¾Ğ¼Ğ° Ğ¿ĞµÑ€ĞµĞ´ ĞºĞ»ÑÑ‡Ğ¾Ğ²Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸
    sql = re.sub(r",\s*(FROM|WHERE|GROUP BY|ORDER BY)", r" \1", sql, flags=re.IGNORECASE)
    # ĞºĞ¾Ğ¼Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½Ğ½ÑĞ¼ Ñ€ÑĞ´ĞºĞ° + ĞºĞ»ÑÑ‡Ğ¾Ğ²Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼
    sql = re.sub(r",\s*\n\s*(FROM|WHERE|GROUP BY|ORDER BY)", r"\n\1", sql, flags=re.IGNORECASE)
    # Ğ¿Ğ¾Ğ´Ğ²Ñ–Ğ¹Ğ½Ñ– ĞºĞ¾Ğ¼Ğ¸
    sql = re.sub(r",\s*,", ", ", sql)
    # ĞºĞ¾Ğ¼Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°ĞºÑ€Ğ¸Ğ²Ğ°ÑÑ‡Ğ¾Ñ Ğ´ÑƒĞ¶ĞºĞ¾Ñ
    sql = re.sub(r",\s*\)", ")", sql)
    return sql


def full_sql_fix(sql: str, date_cols: set) -> str:
    """Ğ„Ğ´Ğ¸Ğ½Ğ¸Ğ¹ pipeline Ğ´Ğ»Ñ Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚Ñƒ SQL."""
    sql_before = sql
    sql = _sanitize_sql_dates(sql, date_cols)
    sql = fix_format_date(sql)
    sql = fix_trailing_commas(sql)
    sql = auto_fix_group_by(sql)

    if sql != sql_before:
        logger.info("[sql fix] SQL was post-processed by full_sql_fix")

    return sql


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# BQ EXECUTOR (with logging)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_cached_query(sql_query: str):
    # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ SQL ÑĞº ĞºĞ»ÑÑ‡ ĞºĞµÑˆÑƒ
    cache_key = get_cache_key(normalize_sql(sql_query))
    now = time.time()

    # cache HIT
    if cache_key in query_cache:
        df, ts = query_cache[cache_key]
        if now - ts < cache_ttl:
            logger.info(
                "[bq] cache HIT key=%s age=%.1fs rows=%d",
                cache_key[:8],
                now - ts,
                len(df),
            )
            return df

    # cache MISS
    logger.info("[bq] cache MISS key=%s", cache_key[:8])
    start = time.perf_counter()
    job = bq_client.query(sql_query)

    try:
        df = job.result().to_dataframe()
        took = time.perf_counter() - start
        logger.info("[bq] OK job_id=%s rows=%d time=%.3fs", job.job_id, len(df), took)

        query_cache[cache_key] = (df.copy(), now)
        # trim cache
        if len(query_cache) > 20:
            oldest_key = min(query_cache, key=lambda k: query_cache[k][1])
            del query_cache[oldest_key]
        return df

    except BadRequest as e:
        msg = getattr(e, "message", str(e))
        logger.exception("[bq] BadRequest job_id=%s : %s", getattr(job, "job_id", "?"), msg)
        raise
    except Exception:
        logger.exception("[bq] FAILED job_id=%s", getattr(job, "job_id", "?"))
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SQL SYNTAX VALIDATION (light checks)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_sql_syntax(sql_query: str):
    errors = []

    window_pattern = r"(?:ROW_NUMBER|RANK|DENSE_RANK|LAG|LEAD)\s*\(\s*\)\s+OVER\s*\([^)]*ORDER\s+BY\s+([^)]+)\)"
    window_matches = re.findall(window_pattern, sql_query, re.IGNORECASE)
    for order_expr in window_matches:
        if "GROUP BY" in sql_query.upper() and not any(
            field in sql_query.split("GROUP BY")[1] for field in order_expr.split(",")
        ):
            errors.append(f"Window ORDER BY Ğ¼Ñ–ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğµ '{order_expr.strip()}', ÑĞºĞµ Ğ½Ğµ Ğ·Ğ³Ñ€ÑƒĞ¿Ğ¾Ğ²Ğ°Ğ½Ğµ")

    if re.search(
        r"WHERE\s+\w+\s+IN\s*\(\s*SELECT.*WHERE.*\w+\.\w+\s*=\s*\w+\.\w+",
        sql_query,
        re.IGNORECASE | re.DOTALL,
    ):
        errors.append("Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ñ– ĞºĞ¾Ñ€ĞµĞ»ÑŒĞ¾Ğ²Ğ°Ğ½Ñ– Ğ¿Ñ–Ğ´Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸, ÑĞºÑ– Ğ½Ğµ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑÑ‚ÑŒÑÑ BigQuery")

    if "STRFTIME" in sql_query.upper():
        errors.append("STRFTIME Ğ½Ğµ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ñ‚ÑŒÑÑ Ğ² BigQuery. Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ FORMAT_DATE")

    return errors


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI matching
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@lru_cache(maxsize=100)
def find_matches_with_ai_cached(instruction: str, semantic_map_str: str):
    smap = json.loads(semantic_map_str)

    context = {}
    for full_key, phrases in smap.items():
        field, value = full_key.split(":")
        context.setdefault(field, {})
        synonyms = []
        for p in phrases:
            synonyms.append(p.get("text", "") if isinstance(p, dict) else str(p))
        context[field][value] = synonyms

    prompt = f"""
Ğ—Ğ½Ğ°Ğ¹Ğ´Ğ¸ ÑĞºÑ– Ğ¿Ğ¾Ğ»Ñ Ğ·Ğ³Ğ°Ğ´ÑƒÑ” ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡, Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‡Ğ¸ ÑĞ¸Ğ½Ğ¾Ğ½Ñ–Ğ¼Ğ¸:

Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ– Ğ¿Ğ¾Ğ»Ñ Ñ‚Ğ° ÑĞ¸Ğ½Ğ¾Ğ½Ñ–Ğ¼Ğ¸:
{json.dumps(context, ensure_ascii=False, indent=2)}

Ğ¢ĞµĞºÑÑ‚ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ°: "{instruction}"

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
- Ğ¯ĞºÑ‰Ğ¾ "Ñ„Ñ–" + "Ñ€ĞµÑ„Ğ°Ğ½Ğ´" â†’ event_type=refund_fee
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ "Ñ€ĞµÑ„Ğ°Ğ½Ğ´" â†’ event_type=refund
- Ğ¯ĞºÑ‰Ğ¾ "Ñ„Ñ–" + "Ñ‡Ğ°Ñ€Ğ´Ğ¶Ğ±ĞµĞº" â†’ event_type=chargeback_fee
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ "Ñ‡Ğ°Ñ€Ğ´Ğ¶Ğ±ĞµĞº" â†’ event_type=chargeback
"""
    try:
        response = model.generate_content(prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        if result == "NONE":
            return []
        matches = []
        for pair in result.split(","):
            if ":" in pair:
                field, value = pair.strip().split(":", 1)
                matches.append((field, value))
        return matches
    except Exception:
        return []


def find_matches_with_ai(instruction, smap):
    return find_matches_with_ai_cached(instruction, json.dumps(smap, sort_keys=True))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Split complex message
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_into_separate_queries(message: str) -> list:
    split_prompt = f"""
Ğ Ğ¾Ğ·Ğ´Ñ–Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ° Ğ½Ğ° Ğ¾ĞºÑ€ĞµĞ¼Ñ– Ğ½ĞµĞ·Ğ°Ğ»ĞµĞ¶Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸. ĞšĞ¾Ğ¶Ğ½Ğµ Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ Ğ°Ğ±Ğ¾ Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚Ğ¸ Ğ¾ĞºÑ€ĞµĞ¼Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¾Ğ¼.

ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ: "{message}"

Ğ—Ğ½Ğ°Ğ¹Ğ´Ğ¸ Ğ²ÑÑ– Ğ¾ĞºÑ€ĞµĞ¼Ñ– Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ/Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ»Ñ–Ñ‡Ğ¸ Ñ—Ñ… Ğ² Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ–:
Ğ—ĞĞŸĞ˜Ğ¢_1: [Ğ¿ĞµÑ€ÑˆĞ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
Ğ—ĞĞŸĞ˜Ğ¢_2: [Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
Ğ—ĞĞŸĞ˜Ğ¢_3: [Ñ‚Ñ€ĞµÑ‚Ñ–Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
"""
    try:
        response = model.generate_content(split_prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        queries = []
        for line in result.split("\n"):
            line = line.strip()
            if line.startswith("Ğ—ĞĞŸĞ˜Ğ¢_"):
                parts = line.split(":", 1)
                if len(parts) == 2 and parts[1].strip():
                    queries.append(parts[1].strip())
        return queries if queries else [message]
    except Exception:
        return [message]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main executors (sync)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_single_query(instruction: str, smap: dict, user_id: str = "unknown") -> str:
    try:
        instruction_part = instruction.strip()
        if not instruction_part:
            return "ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ”. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ."

        logger.info("[execute_single_query] user_id=%s instruction=%s", user_id, instruction_part)

        matched_conditions = find_matches_with_ai(instruction_part, smap)
        for field, value in matched_conditions:
            instruction_part += f" ({field} = '{value}')"
        if matched_conditions:
            logger.debug("[execute_single_query] matched_conditions=%s", matched_conditions)

        rev_schema, cost_schema = get_all_schemas()

        # Ğ·Ğ±ĞµÑ€ĞµĞ¼Ğ¾ Ğ²Ñ–Ğ´Ğ¾Ğ¼Ñ– DATE-Ğ¿Ğ¾Ğ»Ñ (Ñ‰Ğ¾Ğ± Ğ½Ğµ Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ Ñ—Ñ… ÑĞº STRING)
        date_cols = _collect_date_columns(rev_schema) | _collect_date_columns(cost_schema)
        date_cols_list = sorted(list(date_cols))

        sql_prompt = f"""
Ğ’ Ğ½Ğ°Ñ Ñ” Ğ”Ğ’Ğ† Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– Ğ² BigQuery:

1) **REVENUE**: `{REVENUE_TABLE_REF}`
   Ğ¡Ñ…ĞµĞ¼Ğ°:
{json.dumps(rev_schema, indent=2)}

2) **COST**: `{COST_TABLE_REF}`
   Ğ¡Ñ…ĞµĞ¼Ğ°:
{json.dumps(cost_schema, indent=2)}

Ğ—Ğ³ĞµĞ½ĞµÑ€ÑƒĞ¹ Ğ•ĞšĞ¡ĞŸĞ•Ğ Ğ¢ĞĞ˜Ğ™ BigQuery SQL Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ: {instruction_part}

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
- Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ BigQuery SQL.
- ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ STRFTIME; Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ–Ğ² Ğ´Ğ°Ñ‚: FORMAT_DATE('%Y-%m', DATE(...)).
- ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ ĞºĞ¾Ñ€ĞµĞ»ÑŒĞ¾Ğ²Ğ°Ğ½Ñ– Ğ¿Ñ–Ğ´Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸.
- Ğ¯ĞºÑ‰Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ¿Ñ€Ğ¾ Ğ´Ğ¾Ñ…Ñ–Ğ´/Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ñ– â€” REVENUE.
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ¿Ñ€Ğ¾ Ğ²Ğ¸Ñ‚Ñ€Ğ°Ñ‚Ğ¸ â€” COST.
- Ğ”Ğ»Ñ ROAS/Ğ¿Ñ€Ğ¸Ğ±ÑƒÑ‚ĞºÑƒ â€” Ğ°Ğ³Ñ€ĞµĞ³ÑƒĞ¹ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾ Ñ‚Ğ° JOIN.
- Ğ£ REVENUE Ğ´Ğ»Ñ Â«net revenueÂ» â€” ÑÑƒĞ¼ÑƒĞ¹ gross_usd (ÑƒÑÑ– event_type).
- period (12M/1M/6M) â€” Ñ†Ğµ Ñ‚Ğ¸Ğ¿ Ğ¿Ñ–Ğ´Ğ¿Ğ¸ÑĞºĞ¸, Ğ½Ğµ Ñ‡Ğ°Ñ.
- **Ğ§Ğ°ÑĞ¾Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ** Ğ´Ğ»Ñ Ğ²Ñ–Ğ´Ğ½Ğ¾ÑĞ½Ğ¸Ñ… Ğ´Ğ°Ñ‚: CURRENT_DATE('{LOCAL_TZ}').
- **Ğ’Ğ°Ğ¶Ğ»Ğ¸Ğ²Ğ¾**: Ğ¿Ğ¾Ğ»Ñ Ñ‚Ğ¸Ğ¿Ñƒ DATE/DATETIME/TIMESTAMP Ğ½Ğµ Ñ‚Ñ€ĞµĞ±Ğ° Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ ÑĞº STRING.
  Ğ£ Ñ†Ğ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ÑÑ… Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ°Ñ‚: {date_cols_list}. ĞŸĞ¾Ñ€Ñ–Ğ²Ğ½ÑĞ¹ Ñ—Ñ… Ğ±ĞµĞ· PARSE_DATE.
  ĞĞ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, "Ğ²Ñ‡Ğ¾Ñ€Ğ°": posting_date = DATE_SUB(CURRENT_DATE('{LOCAL_TZ}'), INTERVAL 1 DAY).
- ĞŸĞ¾Ğ²ĞµÑ€Ğ½Ğ¸ Ğ»Ğ¸ÑˆĞµ Ñ„Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ SQL Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑĞ½ĞµĞ½ÑŒ.
"""
        response = model.generate_content(sql_prompt, generation_config={"temperature": 0})
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.lower().startswith("sql"):
            sql_query = sql_query[3:].strip()

        logger.info("[sql raw] %s", sql_query)

        # Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ° SQL (Ğ´Ğ°Ñ‚Ğ¸ + FORMAT_DATE + ĞºĞ¾Ğ¼Ğ¸ + GROUP BY)
        sql_query = full_sql_fix(sql_query, date_cols)
        logger.info("[sql fixed] %s", sql_query)

        errs = validate_sql_syntax(sql_query)
        logger.debug("[execute_single_query] generated SQL:\n%s", sql_query)
        if errs:
            logger.warning("[execute_single_query] validation errors: %s", errs)
            return "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ² Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–:**\n" + "\n".join(f"â€¢ {e}" for e in errs)

        try:
            df = execute_cached_query(sql_query)
        except BadRequest as e:
            msg = getattr(e, "message", str(e))[:600]
            out = "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ Ğ´Ğ¾ Ğ±Ğ°Ğ·Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° BigQuery:\n```\n{msg}\n```"
            return out
        except Exception as e:
            msg = (getattr(e, "message", None) or str(e))[:600]
            logger.exception("[execute_single_query] unexpected error")
            out = "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ Ğ´Ğ¾ Ğ±Ğ°Ğ·Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"Ğ”ĞµÑ‚Ğ°Ğ»Ñ–:\n```\n{msg}\n```"
            return out

        if df.empty:
            logger.info("[execute_single_query] empty result")
            return "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹."

        analysis_prompt = f"""
Ğ—Ñ€Ğ¾Ğ±Ğ¸ Ñ‚Ğµ, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ğ² Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ—.
Ğ†Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ: "{instruction_part}"

CSV Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ SQL (ÑƒÑ€Ñ–Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ²Ğ°Ğ¶Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾):
{limited_csv(df)}

Ğ’Ğ¸Ğ¼Ğ¾Ğ³Ğ¸:
- ĞĞµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ğ¹ SQL Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–.
- ĞĞµ Ğ²Ğ¸Ğ³Ğ°Ğ´ÑƒĞ¹ Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ°Ğ±Ğ¾ Ğ´Ğ°Ñ‚ â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ñ‚Ğµ, Ñ‰Ğ¾ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ–.
- Ğ¯ĞºÑ‰Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· â€” Ğ´Ğ¾ 3â€“4 Ñ€ĞµÑ‡ĞµĞ½ÑŒ.
- period (12M/1M/6M) â€” Ñ†Ğµ Ñ‚Ğ¸Ğ¿Ğ¸ Ğ¿Ñ–Ğ´Ğ¿Ğ¸ÑĞ¾Ğº, Ğ½Ğµ Ñ‡Ğ°Ñ.
"""
        analysis_response = model.generate_content(analysis_prompt, generation_config={"temperature": 0})
        return analysis_response.text.strip()

    except Exception as e:
        logger.exception("[execute_single_query] fatal")
        return "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸:\n" + (getattr(e, "message", None) or str(e))


def process_slack_message(message: str, smap: dict, user_id: str = "unknown") -> str:
    try:
        msg = message or ""
        if not msg.strip():
            return "ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ”. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ."

        queries = split_into_separate_queries(msg)
        if len(queries) == 1:
            return execute_single_query(queries[0], smap, user_id=user_id)

        results = []
        for i, q in enumerate(queries, 1):
            try:
                logger.info(
                    "[process_slack_message] user_id=%s part=%d/%d: %s",
                    user_id, i, len(queries), q
                )
                ans = execute_single_query(q, smap, user_id=user_id)
            except Exception:
                logger.exception("[process_slack_message] failed on part %d", i)
                ans = "âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ñ†ÑŒĞ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ."
            results.append((i, q, ans))

        final = f"ğŸ“ **Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(queries)} Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ². Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ñ Ğ½Ğ° ĞºĞ¾Ğ¶ĞµĞ½:**\n\n"
        for i, q, r in results:
            final += f"**ğŸ” Ğ—Ğ°Ğ¿Ğ¸Ñ‚ {i}:** *{q}*\n\n{r}\n\n" + "=" * 60 + "\n\n"
        return final.rstrip("\n=").rstrip()
    except Exception:
        logger.exception("[process_slack_message] fatal")
        return "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ."


def generate_final_conclusion(results: list, original_message: str) -> str:
    try:
        conclusions = []
        for i, q, r in results:
            if "Ğ’Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº:" in r:
                conclusions.append(f"Ğ—Ğ°Ğ¿Ğ¸Ñ‚ {i}: {r.split('Ğ’Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº:')[-1].strip()}")
        if not conclusions:
            return ""
        summary_prompt = f"""
ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² Ğ²ÑÑ–Ñ… Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ² Ğ´Ğ°Ğ¹ Ğ¾Ğ´Ğ¸Ğ½ Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº.

ĞÑ€Ğ¸Ğ³Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğµ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ: "{original_message}"
Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸:
{chr(10).join(conclusions)}

Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ñ–Ğ´ÑÑƒĞ¼Ğ¾Ğº (2â€“4 Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ).
"""
        response = model.generate_content(summary_prompt, generation_config={"temperature": 0})
        return f"ğŸ“‹ **Ğ—ĞĞ“ĞĞ›Ğ¬ĞĞ˜Ğ™ Ğ’Ğ˜Ğ¡ĞĞĞ’ĞĞš:**\n{response.text.strip()}"
    except Exception:
        return f"ğŸ“‹ **Ğ—ĞĞ“ĞĞ›Ğ¬ĞĞ˜Ğ™ Ğ’Ğ˜Ğ¡ĞĞĞ’ĞĞš:**\nĞ’ÑÑ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ĞµĞ½Ğ¾ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾."


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ASYNC-Ğ²ĞµÑ€ÑÑ–Ñ— API (Ğ´Ğ»Ñ FastAPI / Slack handler)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def execute_single_query_async(instruction: str, smap: dict, user_id: str = "unknown") -> str:
    """Async-Ğ¾Ğ±Ğ³Ğ¾Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ execute_single_query."""
    return await _run_in_executor(execute_single_query, instruction, smap, user_id)


async def process_slack_message_async(message: str, smap: dict, user_id: str = "unknown") -> str:
    """Async-Ğ¾Ğ±Ğ³Ğ¾Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ process_slack_message."""
    return await _run_in_executor(process_slack_message, message, smap, user_id)


# Utils
def clear_cache():
    global query_cache, _schema_cache
    query_cache.clear()
    _schema_cache.clear()
    _schema_time.clear()
    find_matches_with_ai_cached.cache_clear()


def get_cache_stats():
    return {
        "query_cache_size": len(query_cache),
        "ai_cache_info": find_matches_with_ai_cached.cache_info(),
    }
