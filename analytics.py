# analytics.py
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
import logging
import traceback
from functools import lru_cache

import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest, GoogleAPIError

import vertexai
from vertexai.preview.generative_models import GenerativeModel

from semantic_map import semantic_map  # ÑĞºÑ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ñ‚Ğ¸ Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENV / LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BQ_PROJECT       = os.getenv("BIGQUERY_PROJECT", "finance-ai-bot-headway")
BQ_DATASET       = os.getenv("BQ_DATASET", "uploads")
BQ_REVENUE_TABLE = os.getenv("BQ_REVENUE_TABLE", "revenue_test_databot")
BQ_COST_TABLE    = os.getenv("BQ_COST_TABLE", "cost_test_databot")
VERTEX_LOCATION  = os.getenv("VERTEX_LOCATION", "europe-west1")

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO))
logger = logging.getLogger("ai-bot")

# ÑĞºÑ‰Ğ¾ TRUE â€” Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ñƒ Slack Ğ´Ğ¾Ğ´Ğ°Ğ¼Ğ¾ Ğ¾Ğ±Ñ€Ñ–Ğ·Ğ°Ğ½Ğ¸Ğ¹ SQL Ñ– Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸
RETURN_SQL_ON_ERROR = os.getenv("RETURN_SQL_ON_ERROR", "false").lower() == "true"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INIT CLIENTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REVENUE_TABLE_REF = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_REVENUE_TABLE}"
COST_TABLE_REF    = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_COST_TABLE}"

# BigQuery
bq_client = bigquery.Client(project=BQ_PROJECT)

# Vertex AI
try:
    vertexai.init(project=BQ_PROJECT, location=VERTEX_LOCATION)
except Exception:
    logger.warning("Vertex init failed; will rely on ambient creds", exc_info=True)
model = GenerativeModel("gemini-2.5-flash")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CACHE (SQL + schemas)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query_cache = {}  # key -> (df, ts)
cache_ttl = 300   # seconds

_schema_cache = {}  # table_ref -> [{"name": ..., "type": ...}]
_schema_time  = {}  # table_ref -> ts


def get_cache_key(query: str) -> str:
    return hashlib.md5(query.encode("utf-8")).hexdigest()


def get_table_schema(table_ref: str, ttl_sec: int = 3600):
    """Return cached schema for table."""
    now = time.time()
    if (
        table_ref not in _schema_cache
        or table_ref not in _schema_time
        or now - _schema_time[table_ref] > ttl_sec
    ):
        schema = bq_client.get_table(table_ref).schema
        _schema_cache[table_ref] = [{"name": f.name, "type": f.field_type} for f in schema]
        _schema_time[table_ref] = now
    return _schema_cache[table_ref]


def get_all_schemas():
    rev_schema = get_table_schema(REVENUE_TABLE_REF)
    try:
        cost_schema = get_table_schema(COST_TABLE_REF)
    except Exception:
        cost_schema = []
    return rev_schema, cost_schema


# Ğ¿Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ½ÑŒĞ¾ Ñ–Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹ (ĞºĞ¾Ñ€Ğ¸ÑĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑˆĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°)
_ = get_all_schemas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# BQ EXECUTOR (with logging)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_cached_query(sql_query: str):
    cache_key = get_cache_key(sql_query)
    now = time.time()

    # cache HIT
    if cache_key in query_cache:
        df, ts = query_cache[cache_key]
        if now - ts < cache_ttl:
            logger.info("[bq] cache HIT key=%s age=%.1fs rows=%d", cache_key[:8], now - ts, len(df))
            return df

    # cache MISS
    logger.info("[bq] cache MISS key=%s", cache_key[:8])
    start = time.perf_counter()
    job = bq_client.query(sql_query)

    try:
        df = job.result().to_dataframe()
        took = time.perf_counter() - start
        logger.info("[bq] OK job_id=%s rows=%d time=%.3fs", job.job_id, len(df), took)

        query_cache[cache_key] = (df.copy(), now)
        # trim cache
        if len(query_cache) > 20:
            oldest_key = min(query_cache, key=lambda k: query_cache[k][1])
            del query_cache[oldest_key]
        return df

    except BadRequest as e:
        msg = getattr(e, "message", str(e))
        logger.exception("[bq] BadRequest job_id=%s : %s", getattr(job, "job_id", "?"), msg)
        raise
    except Exception:
        logger.exception("[bq] FAILED job_id=%s", getattr(job, "job_id", "?"))
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SQL SYNTAX VALIDATION (light checks)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_sql_syntax(sql_query: str):
    errors = []

    window_pattern = r'(?:ROW_NUMBER|RANK|DENSE_RANK|LAG|LEAD)\s*\(\s*\)\s+OVER\s*\([^)]*ORDER\s+BY\s+([^)]+)\)'
    window_matches = re.findall(window_pattern, sql_query, re.IGNORECASE)
    for order_expr in window_matches:
        if 'GROUP BY' in sql_query.upper() and not any(
            field in sql_query.split('GROUP BY')[1] for field in order_expr.split(',')
        ):
            errors.append(f"Window ORDER BY Ğ¼Ñ–ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğµ '{order_expr.strip()}', ÑĞºĞµ Ğ½Ğµ Ğ·Ğ³Ñ€ÑƒĞ¿Ğ¾Ğ²Ğ°Ğ½Ğµ")

    if re.search(r'WHERE\s+\w+\s+IN\s*\(\s*SELECT.*WHERE.*\w+\.\w+\s*=\s*\w+\.\w+', sql_query,
                 re.IGNORECASE | re.DOTALL):
        errors.append("Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ñ– ĞºĞ¾Ñ€ĞµĞ»ÑŒĞ¾Ğ²Ğ°Ğ½Ñ– Ğ¿Ñ–Ğ´Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸, ÑĞºÑ– Ğ½Ğµ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑÑ‚ÑŒÑÑ BigQuery")

    if 'STRFTIME' in sql_query.upper():
        errors.append("STRFTIME Ğ½Ğµ Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ñ‚ÑŒÑÑ Ğ² BigQuery. Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹Ñ‚Ğµ FORMAT_DATE")

    return errors

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI matching
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@lru_cache(maxsize=100)
def find_matches_with_ai_cached(instruction: str, semantic_map_str: str):
    smap = json.loads(semantic_map_str)

    context = {}
    for full_key, phrases in smap.items():
        field, value = full_key.split(":")
        context.setdefault(field, {})
        synonyms = []
        for p in phrases:
            synonyms.append(p.get("text", "") if isinstance(p, dict) else str(p))
        context[field][value] = synonyms

    prompt = f"""
Ğ—Ğ½Ğ°Ğ¹Ğ´Ğ¸ ÑĞºÑ– Ğ¿Ğ¾Ğ»Ñ Ğ·Ğ³Ğ°Ğ´ÑƒÑ” ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡, Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‡Ğ¸ ÑĞ¸Ğ½Ğ¾Ğ½Ñ–Ğ¼Ğ¸:

Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ– Ğ¿Ğ¾Ğ»Ñ Ñ‚Ğ° ÑĞ¸Ğ½Ğ¾Ğ½Ñ–Ğ¼Ğ¸:
{json.dumps(context, ensure_ascii=False, indent=2)}

Ğ¢ĞµĞºÑÑ‚ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ°: "{instruction}"

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
- Ğ¯ĞºÑ‰Ğ¾ "Ñ„Ñ–" + "Ñ€ĞµÑ„Ğ°Ğ½Ğ´" â†’ event_type=refund_fee
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ "Ñ€ĞµÑ„Ğ°Ğ½Ğ´" â†’ event_type=refund
- Ğ¯ĞºÑ‰Ğ¾ "Ñ„Ñ–" + "Ñ‡Ğ°Ñ€Ğ´Ğ¶Ğ±ĞµĞº" â†’ event_type=chargeback_fee
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ "Ñ‡Ğ°Ñ€Ğ´Ğ¶Ğ±ĞµĞº" â†’ event_type=chargeback
"""
    try:
        response = model.generate_content(prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        if result == "NONE":
            return []
        matches = []
        for pair in result.split(','):
            if ':' in pair:
                field, value = pair.strip().split(':', 1)
                matches.append((field, value))
        return matches
    except Exception:
        return []


def find_matches_with_ai(instruction, smap):
    return find_matches_with_ai_cached(instruction, json.dumps(smap, sort_keys=True))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Split complex message
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_into_separate_queries(message: str) -> list:
    split_prompt = f"""
Ğ Ğ¾Ğ·Ğ´Ñ–Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ° Ğ½Ğ° Ğ¾ĞºÑ€ĞµĞ¼Ñ– Ğ½ĞµĞ·Ğ°Ğ»ĞµĞ¶Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸. ĞšĞ¾Ğ¶Ğ½Ğµ Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ Ğ°Ğ±Ğ¾ Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚Ğ¸ Ğ¾ĞºÑ€ĞµĞ¼Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¾Ğ¼.

ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ: "{message}"

Ğ—Ğ½Ğ°Ğ¹Ğ´Ğ¸ Ğ²ÑÑ– Ğ¾ĞºÑ€ĞµĞ¼Ñ– Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ/Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ»Ñ–Ñ‡Ğ¸ Ñ—Ñ… Ğ² Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ–:
Ğ—ĞĞŸĞ˜Ğ¢_1: [Ğ¿ĞµÑ€ÑˆĞ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
Ğ—ĞĞŸĞ˜Ğ¢_2: [Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
Ğ—ĞĞŸĞ˜Ğ¢_3: [Ñ‚Ñ€ĞµÑ‚Ñ–Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚]
"""
    try:
        response = model.generate_content(split_prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        queries = []
        for line in result.split('\n'):
            line = line.strip()
            if line.startswith('Ğ—ĞĞŸĞ˜Ğ¢_'):
                parts = line.split(':', 1)
                if len(parts) == 2 and parts[1].strip():
                    queries.append(parts[1].strip())
        return queries if queries else [message]
    except Exception:
        return [message]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main executors
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_single_query(instruction: str, smap: dict, user_id: str = "unknown") -> str:
    try:
        instruction_part = instruction.strip()
        if not instruction_part:
            return "ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ”. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ."

        logger.info("[execute_single_query] user_id=%s instruction=%s", user_id, instruction_part)

        matched_conditions = find_matches_with_ai(instruction_part, smap)
        for field, value in matched_conditions:
            instruction_part += f" ({field} = '{value}')"
        if matched_conditions:
            logger.debug("[execute_single_query] matched_conditions=%s", matched_conditions)

        rev_schema, cost_schema = get_all_schemas()

        sql_prompt = f"""
Ğ’ Ğ½Ğ°Ñ Ñ” Ğ”Ğ’Ğ† Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– Ğ² BigQuery:

1) **REVENUE**: `{REVENUE_TABLE_REF}`
   Ğ¡Ñ…ĞµĞ¼Ğ°:
{json.dumps(rev_schema, indent=2)}

2) **COST**: `{COST_TABLE_REF}`
   Ğ¡Ñ…ĞµĞ¼Ğ°:
{json.dumps(cost_schema, indent=2)}

Ğ—Ğ³ĞµĞ½ĞµÑ€ÑƒĞ¹ Ğ•ĞšĞ¡ĞŸĞ•Ğ Ğ¢ĞĞ˜Ğ™ BigQuery SQL Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ: {instruction_part}

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
- Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ BigQuery SQL.
- ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ STRFTIME; Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ–Ğ² Ğ´Ğ°Ñ‚: FORMAT_DATE('%Y-%m', DATE(...)).
- ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ ĞºĞ¾Ñ€ĞµĞ»ÑŒĞ¾Ğ²Ğ°Ğ½Ñ– Ğ¿Ñ–Ğ´Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸.
- Ğ¯ĞºÑ‰Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ¿Ñ€Ğ¾ Ğ´Ğ¾Ñ…Ñ–Ğ´/Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ñ– â€” REVENUE.
- Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ¿Ñ€Ğ¾ Ğ²Ğ¸Ñ‚Ñ€Ğ°Ñ‚Ğ¸ â€” COST.
- Ğ”Ğ»Ñ ROAS/Ğ¿Ñ€Ğ¸Ğ±ÑƒÑ‚ĞºÑƒ â€” Ğ°Ğ³Ñ€ĞµĞ³ÑƒĞ¹ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾ Ñ‚Ğ° JOIN.
- Ğ£ REVENUE Ğ´Ğ»Ñ Â«net revenueÂ» â€” ÑÑƒĞ¼ÑƒĞ¹ gross_usd (ÑƒÑÑ– event_type).
- period (12M/1M/6M) â€” Ñ†Ğµ Ñ‚Ğ¸Ğ¿ Ğ¿Ñ–Ğ´Ğ¿Ğ¸ÑĞºĞ¸, Ğ½Ğµ Ñ‡Ğ°Ñ.
- ĞŸĞ¾Ğ²ĞµÑ€Ğ½Ğ¸ Ğ»Ğ¸ÑˆĞµ Ñ„Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ SQL Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑĞ½ĞµĞ½ÑŒ.
"""
        response = model.generate_content(sql_prompt, generation_config={"temperature": 0})
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.lower().startswith("sql"):
            sql_query = sql_query[3:].strip()

        errs = validate_sql_syntax(sql_query)
        logger.debug("[execute_single_query] generated SQL:\n%s", sql_query)
        if errs:
            logger.warning("[execute_single_query] validation errors: %s", errs)
            return "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ² Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–:**\n" + "\n".join(f"â€¢ {e}" for e in errs)

        try:
            df = execute_cached_query(sql_query)
        except BadRequest as e:
            msg = getattr(e, "message", str(e))[:600]
            out = "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ Ğ´Ğ¾ Ğ±Ğ°Ğ·Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° BigQuery:\n```\n{msg}\n```"
            return out
        except Exception as e:
            msg = (getattr(e, "message", None) or str(e))[:600]
            logger.exception("[execute_single_query] unexpected error")
            out = "âŒ **ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ Ğ´Ğ¾ Ğ±Ğ°Ğ·Ğ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"Ğ”ĞµÑ‚Ğ°Ğ»Ñ–:\n```\n{msg}\n```"
            return out

        if df.empty:
            logger.info("[execute_single_query] empty result")
            return "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹."

        analysis_prompt = f"""
Ğ—Ñ€Ğ¾Ğ±Ğ¸ Ñ‚Ğµ, Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ğ² Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ—.
Ğ†Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ: "{instruction_part}"

CSV Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ SQL:
{df.to_csv(index=False)}

Ğ’Ğ¸Ğ¼Ğ¾Ğ³Ğ¸:
- ĞĞµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ğ¹ SQL Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–.
- ĞĞµ Ğ²Ğ¸Ğ³Ğ°Ğ´ÑƒĞ¹ Ğ´Ğ°Ğ½Ğ¸Ñ… Ğ°Ğ±Ğ¾ Ğ´Ğ°Ñ‚ â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ñ‚Ğµ, Ñ‰Ğ¾ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ–.
- Ğ¯ĞºÑ‰Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· â€” Ğ´Ğ¾ 3â€“4 Ñ€ĞµÑ‡ĞµĞ½ÑŒ.
- period (12M/1M/6M) â€” Ñ†Ğµ Ñ‚Ğ¸Ğ¿Ğ¸ Ğ¿Ñ–Ğ´Ğ¿Ğ¸ÑĞ¾Ğº, Ğ½Ğµ Ñ‡Ğ°Ñ.
"""
        analysis_response = model.generate_content(analysis_prompt, generation_config={"temperature": 0})
        return analysis_response.text.strip()

    except Exception as e:
        logger.exception("[execute_single_query] fatal")
        return "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸:\n" + (getattr(e, "message", None) or str(e))


def process_slack_message(message: str, smap: dict, user_id: str = "unknown") -> str:
    try:
        if not message.strip():
            return "ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ”. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ."
        queries = split_into_separate_queries(message)
        if len(queries) == 1:
            return execute_single_query(queries[0], smap, user_id=user_id)

        results = []
        for i, q in enumerate(queries, 1):
            logger.info("[process_slack_message] user_id=%s part=%d/%d: %s", user_id, i, len(queries), q)
            results.append((i, q, execute_single_query(q, smap, user_id=user_id)))

        final = f"ğŸ“ **Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(queries)} Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ². Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ñ Ğ½Ğ° ĞºĞ¾Ğ¶ĞµĞ½:**\n\n"
        for i, q, r in results:
            final += f"**ğŸ” Ğ—Ğ°Ğ¿Ğ¸Ñ‚ {i}:** *{q}*\n\n{r}\n\n" + "="*60 + "\n\n"
        return final.rstrip("\n=").rstrip()
    except Exception:
        logger.exception("[process_slack_message] fatal")
        return "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ."


def generate_final_conclusion(results: list, original_message: str) -> str:
    try:
        conclusions = []
        for i, q, r in results:
            if "Ğ’Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº:" in r:
                conclusions.append(f"Ğ—Ğ°Ğ¿Ğ¸Ñ‚ {i}: {r.split('Ğ’Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº:')[-1].strip()}")
        if not conclusions:
            return ""
        summary_prompt = f"""
ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² Ğ²ÑÑ–Ñ… Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ² Ğ´Ğ°Ğ¹ Ğ¾Ğ´Ğ¸Ğ½ Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº.

ĞÑ€Ğ¸Ğ³Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğµ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ: "{original_message}"
Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸:
{chr(10).join(conclusions)}

Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ¹ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ñ–Ğ´ÑÑƒĞ¼Ğ¾Ğº (2â€“4 Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ).
"""
        response = model.generate_content(summary_prompt, generation_config={"temperature": 0})
        return f"ğŸ“‹ **Ğ—ĞĞ“ĞĞ›Ğ¬ĞĞ˜Ğ™ Ğ’Ğ˜Ğ¡ĞĞĞ’ĞĞš:**\n{response.text.strip()}"
    except Exception:
        return f"ğŸ“‹ **Ğ—ĞĞ“ĞĞ›Ğ¬ĞĞ˜Ğ™ Ğ’Ğ˜Ğ¡ĞĞĞ’ĞĞš:**\nĞ’ÑÑ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ĞµĞ½Ğ¾ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾."

# Utils
def clear_cache():
    global query_cache, _schema_cache
    query_cache.clear()
    _schema_cache.clear()
    _schema_time.clear()
    find_matches_with_ai_cached.cache_clear()

def get_cache_stats():
    return {
        "query_cache_size": len(query_cache),
        "ai_cache_info": find_matches_with_ai_cached.cache_info()
    }
