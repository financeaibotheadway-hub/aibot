# analytics/analytics_core.py
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
import logging
import traceback
from functools import lru_cache

import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest, GoogleAPIError

import vertexai
from vertexai.preview.generative_models import GenerativeModel

from semantic_map import semantic_map  # —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ENV / LOGGING
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BQ_PROJECT       = os.getenv("BIGQUERY_PROJECT", "finance-ai-bot-headway")
BQ_DATASET       = os.getenv("BQ_DATASET", "uploads")
BQ_REVENUE_TABLE = os.getenv("BQ_REVENUE_TABLE", "revenue_test_databot")
BQ_COST_TABLE    = os.getenv("BQ_COST_TABLE", "cost_test_databot")
VERTEX_LOCATION  = os.getenv("VERTEX_LOCATION", "europe-west1")
LOCAL_TZ         = os.getenv("LOCAL_TZ", "Europe/Kyiv")     # >>> TZ –¥–ª—è –¥–∞—Ç

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO))
logger = logging.getLogger("ai-bot")

# —è–∫—â–æ TRUE ‚Äî —É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É Slack –¥–æ–¥–∞–º–æ –æ–±—Ä—ñ–∑–∞–Ω–∏–π SQL —ñ —Ç–µ–∫—Å—Ç –ø–æ–º–∏–ª–∫–∏
RETURN_SQL_ON_ERROR = os.getenv("RETURN_SQL_ON_ERROR", "false").lower() == "true"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# INIT CLIENTS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
REVENUE_TABLE_REF = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_REVENUE_TABLE}"
COST_TABLE_REF    = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_COST_TABLE}"

# BigQuery
bq_client = bigquery.Client(project=BQ_PROJECT)

# Vertex AI
try:
    vertexai.init(project=BQ_PROJECT, location=VERTEX_LOCATION)
except Exception:
    logger.warning("Vertex init failed; will rely on ambient creds", exc_info=True)
model = GenerativeModel("gemini-2.5-flash")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CACHE (SQL + schemas)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
query_cache = {}  # key -> (df, ts)
cache_ttl = 300   # seconds

_schema_cache = {}  # table_ref -> [{"name": ..., "type": ...}]
_schema_time  = {}  # table_ref -> ts


def get_cache_key(query: str) -> str:
    return hashlib.md5(query.encode("utf-8")).hexdigest()


def get_table_schema(table_ref: str, ttl_sec: int = 3600):
    """Return cached schema for table."""
    now = time.time()
    if (
        table_ref not in _schema_cache
        or table_ref not in _schema_time
        or now - _schema_time[table_ref] > ttl_sec
    ):
        schema = bq_client.get_table(table_ref).schema
        _schema_cache[table_ref] = [{"name": f.name, "type": f.field_type} for f in schema]
        _schema_time[table_ref] = now
    return _schema_cache[table_ref]


def get_all_schemas():
    rev_schema = get_table_schema(REVENUE_TABLE_REF)
    try:
        cost_schema = get_table_schema(COST_TABLE_REF)
    except Exception:
        cost_schema = []
    return rev_schema, cost_schema


# –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–π (–∫–æ—Ä–∏—Å–Ω–æ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞)
_ = get_all_schemas()

# >>> —É—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è –¥–∞—Ç
def _collect_date_columns(schema_list):
    """–ü–æ–≤–µ—Ä—Ç–∞—î –º–Ω–æ–∂–∏–Ω—É –ø–æ–ª—ñ–≤, —è–∫—ñ –º–∞—é—Ç—å DATE/DATETIME/TIMESTAMP (—â–æ–± —ó—Ö –Ω–µ –ø–∞—Ä—Å–∏–ª–∏ —è–∫ STRING)."""
    return {
        f["name"]
        for f in schema_list
        if f.get("type") in ("DATE", "DATETIME", "TIMESTAMP")
    }


def _sanitize_sql_dates(sql_query: str, date_columns: set) -> str:
    """
    –ü–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∞ SQL: –ø—Ä–∏–±–∏—Ä–∞—î PARSE_DATE(..., <date_col>) —Ç–∞ SAFE.PARSE_DATE –¥–ª—è –≤—ñ–¥–æ–º–∏—Ö DATE-–ø–æ–ª—ñ–≤,
    –ø—ñ–¥—Å—Ç–∞–≤–ª—è—î CURRENT_DATE('<tz>') —è–∫—â–æ –±–µ–∑ TZ.
    """
    original = sql_query

    # 1) CURRENT_DATE() / CURRENT_DATE  ‚Üí CURRENT_DATE('Europe/Kyiv')
    #    (–Ω–µ —á—ñ–ø–∞—î, —è–∫—â–æ TZ —É–∂–µ –∑–∞–¥–∞–Ω–∏–π)
    sql_query = re.sub(
        r"\bCURRENT_DATE\s*\(\s*\)",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )
    sql_query = re.sub(
        r"\bCURRENT_DATE\b(?!\s*\()",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )

    # 2) –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—ñ–¥–æ–º–æ–≥–æ DATE-–ø–æ–ª—è –ø—Ä–∏–±–∏—Ä–∞—î–º–æ PARSE_DATE('%Y-%m-%d', col) -> col
    for col in sorted(date_columns, key=len, reverse=True):
        # –∑ —ñ–º–µ–Ω–∞–º–∏-–∞–ª—ñ–∞—Å–∞–º–∏ —Ç–∏–ø—É t.posting_date –∞–±–æ `posting_date`
        pattern_plain = rf"PARSE_DATE\(\s*'[^']+'\s*,\s*(`?[\w\.]+`?)\s*\)"
        def _repl_plain(m):
            inner = m.group(1)
            # –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–±—ñ–≥–∞—î—Ç—å—Å—è –∑ –∫–æ–ª–æ–Ω–æ—é (–∞–±–æ –∑ —Å—É—Ñ—ñ–∫—Å–æ–º .col)
            inner_clean = inner.strip("`")
            if inner_clean.endswith(f".{col}") or inner_clean == col:
                return inner
            return m.group(0)
        sql_query = re.sub(pattern_plain, _repl_plain, sql_query, flags=re.IGNORECASE)

        # SAFE.PARSE_DATE(...) -> CAST(col AS DATE)
        pattern_safe = rf"SAFE\.PARSE_DATE\(\s*'[^']+'\s*,\s*(`?[\w\.]+`?)\s*\)"
        def _repl_safe(m):
            inner = m.group(1)
            inner_clean = inner.strip("`")
            if inner_clean.endswith(f".{col}") or inner_clean == col:
                return f"CAST({inner} AS DATE)"
            return m.group(0)
        sql_query = re.sub(pattern_safe, _repl_safe, sql_query, flags=re.IGNORECASE)

    if sql_query != original:
        logger.info("[sanitize] SQL was sanitized for date handling")

    return sql_query
# <<< –∫—ñ–Ω–µ—Ü—å —É—Ç–∏–ª—ñ—Ç

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# BQ EXECUTOR (with logging)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def execute_cached_query(sql_query: str):
    cache_key = get_cache_key(sql_query)
    now = time.time()

    # cache HIT
    if cache_key in query_cache:
        df, ts = query_cache[cache_key]
        if now - ts < cache_ttl:
            logger.info("[bq] cache HIT key=%s age=%.1fs rows=%d", cache_key[:8], now - ts, len(df))
            return df

    # cache MISS
    logger.info("[bq] cache MISS key=%s", cache_key[:8])
    start = time.perf_counter()
    job = bq_client.query(sql_query)

    try:
        df = job.result().to_dataframe()
        took = time.perf_counter() - start
        logger.info("[bq] OK job_id=%s rows=%d time=%.3fs", job.job_id, len(df), took)

        query_cache[cache_key] = (df.copy(), now)
        # trim cache
        if len(query_cache) > 20:
            oldest_key = min(query_cache, key=lambda k: query_cache[k][1])
            del query_cache[oldest_key]
        return df

    except BadRequest as e:
        msg = getattr(e, "message", str(e))
        logger.exception("[bq] BadRequest job_id=%s : %s", getattr(job, "job_id", "?"), msg)
        raise
    except Exception:
        logger.exception("[bq] FAILED job_id=%s", getattr(job, "job_id", "?"))
        raise

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SQL SYNTAX VALIDATION (light checks)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def validate_sql_syntax(sql_query: str):
    errors = []

    window_pattern = r'(?:ROW_NUMBER|RANK|DENSE_RANK|LAG|LEAD)\s*\(\s*\)\s+OVER\s*\([^)]*ORDER\s+BY\s+([^)]+)\)'
    window_matches = re.findall(window_pattern, sql_query, re.IGNORECASE)
    for order_expr in window_matches:
        if 'GROUP BY' in sql_query.upper() and not any(
            field in sql_query.split('GROUP BY')[1] for field in order_expr.split(',')
        ):
            errors.append(f"Window ORDER BY –º—ñ—Å—Ç–∏—Ç—å –ø–æ–ª–µ '{order_expr.strip()}', —è–∫–µ –Ω–µ –∑–≥—Ä—É–ø–æ–≤–∞–Ω–µ")

    if re.search(r'WHERE\s+\w+\s+IN\s*\(\s*SELECT.*WHERE.*\w+\.\w+\s*=\s*\w+\.\w+', sql_query,
                 re.IGNORECASE | re.DOTALL):
        errors.append("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –ø—ñ–¥–∑–∞–ø–∏—Ç–∏, —è–∫—ñ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è BigQuery")

    if 'STRFTIME' in sql_query.upper():
        errors.append("STRFTIME –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –≤ BigQuery. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ FORMAT_DATE")

    return errors

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# AI matching
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=100)
def find_matches_with_ai_cached(instruction: str, semantic_map_str: str):
    smap = json.loads(semantic_map_str)

    context = {}
    for full_key, phrases in smap.items():
        field, value = full_key.split(":")
        context.setdefault(field, {})
        synonyms = []
        for p in phrases:
            synonyms.append(p.get("text", "") if isinstance(p, dict) else str(p))
        context[field][value] = synonyms

    prompt = f"""
–ó–Ω–∞–π–¥–∏ —è–∫—ñ –ø–æ–ª—è –∑–≥–∞–¥—É—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Å–∏–Ω–æ–Ω—ñ–º–∏:

–î–æ—Å—Ç—É–ø–Ω—ñ –ø–æ–ª—è —Ç–∞ —Å–∏–Ω–æ–Ω—ñ–º–∏:
{json.dumps(context, ensure_ascii=False, indent=2)}

–¢–µ–∫—Å—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: "{instruction}"

–ü—Ä–∞–≤–∏–ª–∞:
- –Ø–∫—â–æ "—Ñ—ñ" + "—Ä–µ—Ñ–∞–Ω–¥" ‚Üí event_type=refund_fee
- –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ "—Ä–µ—Ñ–∞–Ω–¥" ‚Üí event_type=refund
- –Ø–∫—â–æ "—Ñ—ñ" + "—á–∞—Ä–¥–∂–±–µ–∫" ‚Üí event_type=chargeback_fee
- –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ "—á–∞—Ä–¥–∂–±–µ–∫" ‚Üí event_type=chargeback
"""
    try:
        response = model.generate_content(prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        if result == "NONE":
            return []
        matches = []
        for pair in result.split(','):
            if ':' in pair:
                field, value = pair.strip().split(':', 1)
                matches.append((field, value))
        return matches
    except Exception:
        return []


def find_matches_with_ai(instruction, smap):
    return find_matches_with_ai_cached(instruction, json.dumps(smap, sort_keys=True))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Split complex message
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def split_into_separate_queries(message: str) -> list:
    split_prompt = f"""
–†–æ–∑–¥—ñ–ª–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–∞ –æ–∫—Ä–µ–º—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –∑–∞–ø–∏—Ç–∏. –ö–æ–∂–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –∑–∞–≤–¥–∞–Ω–Ω—è –º–∞—î –±—É—Ç–∏ –æ–∫—Ä–µ–º–∏–º –∑–∞–ø–∏—Ç–æ–º.

–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: "{message}"

–ó–Ω–∞–π–¥–∏ –≤—Å—ñ –æ–∫—Ä–µ–º—ñ –ø–∏—Ç–∞–Ω–Ω—è/–∑–∞–≤–¥–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–ª—ñ—á–∏ —ó—Ö –≤ —Ç–∞–∫–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ:
–ó–ê–ü–ò–¢_1: [–ø–µ—Ä—à–∏–π –∑–∞–ø–∏—Ç]
–ó–ê–ü–ò–¢_2: [–¥—Ä—É–≥–∏–π –∑–∞–ø–∏—Ç]
–ó–ê–ü–ò–¢_3: [—Ç—Ä–µ—Ç—ñ–π –∑–∞–ø–∏—Ç]
"""
    try:
        response = model.generate_content(split_prompt, generation_config={"temperature": 0})
        result = response.text.strip()
        queries = []
        for line in result.split('\n'):
            line = line.strip()
            if line.startswith('–ó–ê–ü–ò–¢_'):
                parts = line.split(':', 1)
                if len(parts) == 2 and parts[1].strip():
                    queries.append(parts[1].strip())
        return queries if queries else [message]
    except Exception:
        return [message]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main executors
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def execute_single_query(instruction: str, smap: dict, user_id: str = "unknown") -> str:
    try:
        instruction_part = instruction.strip()
        if not instruction_part:
            return "–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—î. –ù–∞–ø–∏—à–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é."

        logger.info("[execute_single_query] user_id=%s instruction=%s", user_id, instruction_part)

        matched_conditions = find_matches_with_ai(instruction_part, smap)
        for field, value in matched_conditions:
            instruction_part += f" ({field} = '{value}')"
        if matched_conditions:
            logger.debug("[execute_single_query] matched_conditions=%s", matched_conditions)

        rev_schema, cost_schema = get_all_schemas()

        # >>> –∑–±–µ—Ä–µ–º–æ –≤—ñ–¥–æ–º—ñ DATE-–ø–æ–ª—è (—â–æ–± –Ω–µ –ø–∞—Ä—Å–∏—Ç–∏ —ó—Ö —è–∫ STRING)
        date_cols = _collect_date_columns(rev_schema) | _collect_date_columns(cost_schema)
        date_cols_list = sorted(list(date_cols))
        # <<<

        sql_prompt = f"""
–í –Ω–∞—Å —î –î–í–Ü —Ç–∞–±–ª–∏—Ü—ñ –≤ BigQuery:

1) **REVENUE**: `{REVENUE_TABLE_REF}`
   –°—Ö–µ–º–∞:
{json.dumps(rev_schema, indent=2)}

2) **COST**: `{COST_TABLE_REF}`
   –°—Ö–µ–º–∞:
{json.dumps(cost_schema, indent=2)}

–ó–≥–µ–Ω–µ—Ä—É–π –ï–ö–°–ü–ï–†–¢–ù–ò–ô BigQuery SQL –¥–ª—è –∑–∞–≤–¥–∞–Ω–Ω—è: {instruction_part}

–ü—Ä–∞–≤–∏–ª–∞:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¢–Ü–õ–¨–ö–ò BigQuery SQL.
- –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π STRFTIME; –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –¥–∞—Ç: FORMAT_DATE('%Y-%m', DATE(...)).
- –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –ø—ñ–¥–∑–∞–ø–∏—Ç–∏.
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ –¥–æ—Ö—ñ–¥/–ø—Ä–æ–¥–∞–∂—ñ ‚Äî REVENUE.
- –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ –≤–∏—Ç—Ä–∞—Ç–∏ ‚Äî COST.
- –î–ª—è ROAS/–ø—Ä–∏–±—É—Ç–∫—É ‚Äî –∞–≥—Ä–µ–≥—É–π –æ–∫—Ä–µ–º–æ —Ç–∞ JOIN.
- –£ REVENUE –¥–ª—è ¬´net revenue¬ª ‚Äî —Å—É–º—É–π gross_usd (—É—Å—ñ event_type).
- period (12M/1M/6M) ‚Äî —Ü–µ —Ç–∏–ø –ø—ñ–¥–ø–∏—Å–∫–∏, –Ω–µ —á–∞—Å.
- **–ß–∞—Å–æ–≤–∏–π –ø–æ—è—Å** –¥–ª—è –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö –¥–∞—Ç: CURRENT_DATE('{LOCAL_TZ}').
- **–í–∞–∂–ª–∏–≤–æ**: –ø–æ–ª—è —Ç–∏–ø—É DATE/DATETIME/TIMESTAMP –Ω–µ —Ç—Ä–µ–±–∞ –ø–∞—Ä—Å–∏—Ç–∏ —è–∫ STRING.
  –£ —Ü–∏—Ö —Ç–∞–±–ª–∏—Ü—è—Ö –ø–æ–ª—è –¥–∞—Ç: {date_cols_list}. –ü–æ—Ä—ñ–≤–Ω—é–π —ó—Ö –±–µ–∑ PARSE_DATE.
  –ù–∞–ø—Ä–∏–∫–ª–∞–¥, "–≤—á–æ—Ä–∞": posting_date = DATE_SUB(CURRENT_DATE('{LOCAL_TZ}'), INTERVAL 1 DAY).
- –ü–æ–≤–µ—Ä–Ω–∏ –ª–∏—à–µ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π SQL –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å.
"""
        response = model.generate_content(sql_prompt, generation_config={"temperature": 0})
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.lower().startswith("sql"):
            sql_query = sql_query[3:].strip()

        # >>> –ø–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∞ SQL (–ø—Ä–∏–±—Ä–∞—Ç–∏ PARSE_DATE –Ω–∞ DATE-–ø–æ–ª—è—Ö, –¥–æ–¥–∞—Ç–∏ TZ)
        sql_query = _sanitize_sql_dates(sql_query, date_cols)
        # <<<

        # >>> FIX dangling UNION / UNION ALL at end of SQL
        sql_query = re.sub(
            r'(UNION|UNION ALL)\s*(--.*)?$',
            '',
            sql_query.strip(),
            flags=re.IGNORECASE | re.MULTILINE
        )

        # >>> FIX: –≤–∏–¥–∞–ª–∏—Ç–∏ –ª—ñ–Ω—ñ—ó, –¥–µ –∑–∞–ª–∏—à–∏–≤—Å—è –ª–∏—à–µ UNION / UNION ALL
        lines = sql_query.splitlines()
        while lines and re.match(r'\s*(UNION|UNION ALL)\s*$', lines[-1], flags=re.IGNORECASE):
            lines.pop()
        sql_query = "\n".join(lines)
        # <<<

        errs = validate_sql_syntax(sql_query)

        errs = validate_sql_syntax(sql_query)
        logger.debug("[execute_single_query] generated SQL:\n%s", sql_query)
        if errs:
            logger.warning("[execute_single_query] validation errors: %s", errs)
            return "‚ùå **–ü–æ–º–∏–ª–∫–∞ –≤ –∑–∞–ø–∏—Ç—ñ:**\n" + "\n".join(f"‚Ä¢ {e}" for e in errs)

        try:
            df = execute_cached_query(sql_query)
        except BadRequest as e:
            msg = getattr(e, "message", str(e))[:600]
            out = "‚ùå **–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ –∑–∞–ø–∏—Ç—É –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"–ü–æ–º–∏–ª–∫–∞ BigQuery:\n```\n{msg}\n```"
            return out
        except Exception as e:
            msg = (getattr(e, "message", None) or str(e))[:600]
            logger.exception("[execute_single_query] unexpected error")
            out = "‚ùå **–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ –∑–∞–ø–∏—Ç—É –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.**\n"
            if RETURN_SQL_ON_ERROR:
                out += f"SQL:\n```sql\n{sql_query[:1500]}\n```\n"
            out += f"–î–µ—Ç–∞–ª—ñ:\n```\n{msg}\n```"
            return out

        if df.empty:
            logger.info("[execute_single_query] empty result")
            return "–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–±–ª–∏—Ü—ñ –ø–æ—Ä–æ–∂–Ω—ñ–π."

        analysis_prompt = f"""
–ó—Ä–æ–±–∏ —Ç–µ, —â–æ –ø—Ä–æ—Å–∏—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó.
–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è: "{instruction_part}"

CSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç SQL:
{df.to_csv(index=False)}

–í–∏–º–æ–≥–∏:
- –ù–µ –ø–æ–≤–µ—Ä—Ç–∞–π SQL —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
- –ù–µ –≤–∏–≥–∞–¥—É–π –¥–∞–Ω–∏—Ö –∞–±–æ –¥–∞—Ç ‚Äî —Ç—ñ–ª—å–∫–∏ —Ç–µ, —â–æ –≤ —Ç–∞–±–ª–∏—Ü—ñ.
- –Ø–∫—â–æ –ø—Ä–æ—Å–∏–ª–∏ –∞–Ω–∞–ª—ñ–∑ ‚Äî –¥–æ 3‚Äì4 —Ä–µ—á–µ–Ω—å.
- period (12M/1M/6M) ‚Äî —Ü–µ —Ç–∏–ø–∏ –ø—ñ–¥–ø–∏—Å–æ–∫, –Ω–µ —á–∞—Å.
"""
        analysis_response = model.generate_content(analysis_prompt, generation_config={"temperature": 0})
        return analysis_response.text.strip()

    except Exception as e:
        logger.exception("[execute_single_query] fatal")
        return "–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏:\n" + (getattr(e, "message", None) or str(e))


def process_slack_message(message: str, smap: dict, user_id: str = "unknown") -> str:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫—É –±—É–¥–µ–º–æ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –∑ Slack (—á–µ—Ä–µ–∑ –æ–±–≥–æ—Ä—Ç–∫—É run_analysis).
    """
    try:
        if not message.strip():
            return "–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—î. –ù–∞–ø–∏—à–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é."
        queries = split_into_separate_queries(message)
        if len(queries) == 1:
            return execute_single_query(queries[0], smap, user_id=user_id)

        results = []
        for i, q in enumerate(queries, 1):
            logger.info("[process_slack_message] user_id=%s part=%d/%d: %s", user_id, i, len(queries), q)
            results.append((i, q, execute_single_query(q, smap, user_id=user_id)))

        final = f"üìù **–ó–Ω–∞–π–¥–µ–Ω–æ {len(queries)} –∑–∞–ø–∏—Ç—ñ–≤. –í—ñ–¥–ø–æ–≤—ñ–¥–∞—é –Ω–∞ –∫–æ–∂–µ–Ω:**\n\n"
        for i, q, r in results:
            final += f"**üîç –ó–∞–ø–∏—Ç {i}:** *{q}*\n\n{r}\n\n" + "="*60 + "\n\n"
        return final.rstrip("\n=").rstrip()
    except Exception:
        logger.exception("[process_slack_message] fatal")
        return "–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."


def generate_final_conclusion(results: list, original_message: str) -> str:
    try:
        conclusions = []
        for i, q, r in results:
            if "–í–∏—Å–Ω–æ–≤–æ–∫:" in r:
                conclusions.append(f"–ó–∞–ø–∏—Ç {i}: {r.split('–í–∏—Å–Ω–æ–≤–æ–∫:')[-1].strip()}")
        if not conclusions:
            return ""
        summary_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤—Å—ñ—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–∞–π –æ–¥–∏–Ω –∑–∞–≥–∞–ª—å–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫.

–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: "{original_message}"
–†–µ–∑—É–ª—å—Ç–∞—Ç–∏:
{chr(10).join(conclusions)}

–°—Ñ–æ—Ä–º—É–π –∫–æ—Ä–æ—Ç–∫–∏–π –ø—ñ–¥—Å—É–º–æ–∫ (2‚Äì4 —Ä–µ—á–µ–Ω–Ω—è).
"""
        response = model.generate_content(summary_prompt, generation_config={"temperature": 0})
        return f"üìã **–ó–ê–ì–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö:**\n{response.text.strip()}"
    except Exception:
        return f"üìã **–ó–ê–ì–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö:**\n–í—Å—ñ –∑–∞–ø–∏—Ç–∏ –æ–±—Ä–æ–±–ª–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ."


# Utils
def clear_cache():
    global query_cache, _schema_cache
    query_cache.clear()
    _schema_cache.clear()
    _schema_time.clear()
    find_matches_with_ai_cached.cache_clear()


def get_cache_stats():
    return {
        "query_cache_size": len(query_cache),
        "ai_cache_info": find_matches_with_ai_cached.cache_info()
    }
    
def run_analysis(message: str,
                 semantic_map_override: dict | None = None,
                 user_id: str = "unknown") -> str:
    """
    –ì–æ–ª–æ–≤–Ω–∏–π entry point, —è–∫–∏–π –≤–∏–∫–ª–∏–∫–∞—î process_slack_message.
    semantic_map_override ‚Äì –º–æ–∂–µ–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Å–≤—ñ–π semantic_map, —è–∫—â–æ —Ç—Ä–µ–±–∞,
    —ñ–Ω–∞–∫—à–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω–∏–π semantic_map –∑ —Ü—å–æ–≥–æ –º–æ–¥—É–ª—è.
    """
    smap = semantic_map_override or semantic_map
    return process_slack_message(message, smap, user_id=user_id)
