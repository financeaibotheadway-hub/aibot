# analytics/analytics_core.py
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import hashlib
import logging
import traceback
from functools import lru_cache

import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest, GoogleAPIError

import vertexai
from vertexai.preview.generative_models import GenerativeModel

from semantic_map import semantic_map

# >>>>>>>>>>>> INTEGRATION (NEW)
from analytics.metric_loader import get_metrics
from analytics.metric_parser import detect_metric
from analytics.trend_analysis import run_trend_analysis
# <<<<<<<<<<<< INTEGRATION END


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENV / LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BQ_PROJECT       = os.getenv("BIGQUERY_PROJECT", "finance-ai-bot-headway")
BQ_DATASET       = os.getenv("BQ_DATASET", "uploads")
BQ_REVENUE_TABLE = os.getenv("BQ_REVENUE_TABLE", "revenue_test_databot")
BQ_COST_TABLE    = os.getenv("BQ_COST_TABLE", "cost_test_databot")
VERTEX_LOCATION  = os.getenv("VERTEX_LOCATION", "europe-west1")
LOCAL_TZ         = os.getenv("LOCAL_TZ", "Europe/Kyiv")

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO))
logger = logging.getLogger("ai-bot")

RETURN_SQL_ON_ERROR = os.getenv("RETURN_SQL_ON_ERROR", "false").lower() == "true"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INIT CLIENTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REVENUE_TABLE_REF = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_REVENUE_TABLE}"
COST_TABLE_REF    = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_COST_TABLE}"

bq_client = bigquery.Client(project=BQ_PROJECT)

try:
    vertexai.init(project=BQ_PROJECT, location=VERTEX_LOCATION)
except Exception:
    logger.warning("Vertex init failed", exc_info=True)

model = GenerativeModel("gemini-2.5-flash")

query_cache = {}
cache_ttl = 300

_schema_cache = {}
_schema_time  = {}


def get_cache_key(query: str) -> str:
    return hashlib.md5(query.encode("utf-8")).hexdigest()


def get_table_schema(table_ref: str, ttl_sec: int = 3600):
    now = time.time()
    if table_ref not in _schema_cache or now - _schema_time.get(table_ref, 0) > ttl_sec:
        schema = bq_client.get_table(table_ref).schema
        _schema_cache[table_ref] = [{"name": c.name, "type": c.field_type} for c in schema]
        _schema_time[table_ref] = now
    return _schema_cache[table_ref]


def get_all_schemas():
    rev_schema = get_table_schema(REVENUE_TABLE_REF)
    try:
        cost_schema = get_table_schema(COST_TABLE_REF)
    except Exception:
        cost_schema = []
    return rev_schema, cost_schema


# >>> preload
_ = get_all_schemas()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DATE TOOLS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _collect_date_columns(schema_list):
    return {
        f["name"]
        for f in schema_list
        if f.get("type") in ("DATE", "DATETIME", "TIMESTAMP")
    }


def _sanitize_sql_dates(sql_query: str, date_columns: set) -> str:
    sql_original = sql_query

    sql_query = re.sub(
        r"\bCURRENT_DATE\s*\(\s*\)",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )

    sql_query = re.sub(
        r"\bCURRENT_DATE\b(?!\s*\()",
        f"CURRENT_DATE('{LOCAL_TZ}')",
        sql_query,
        flags=re.IGNORECASE,
    )

    # Remove PARSE_DATE around existing DATE fields
    for col in date_columns:
        p1 = rf"PARSE_DATE\(\s*'[^']+'\s*,\s*(`?[\w\.]+`?)\s*\)"

        def repl1(m):
            inner = m.group(1)
            clean = inner.strip("`")
            if clean.endswith(f".{col}") or clean == col:
                return inner
            return m.group(0)

        sql_query = re.sub(p1, repl1, sql_query, flags=re.IGNORECASE)

    return sql_query


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXECUTOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_cached_query(sql_query: str):
    cache_key = get_cache_key(sql_query)
    now = time.time()

    if cache_key in query_cache:
        df, ts = query_cache[cache_key]
        if now - ts < cache_ttl:
            return df

    job = bq_client.query(sql_query)
    df = job.result().to_dataframe()

    query_cache[cache_key] = (df.copy(), now)
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI FIELD MATCHING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@lru_cache(maxsize=100)
def find_matches_with_ai_cached(instruction: str, smap_json: str):
    smap = json.loads(smap_json)

    prompt = f"""
Ğ—Ğ½Ğ°Ğ¹Ğ´Ğ¸ Ğ²ÑÑ– Ğ¿Ğ¾Ğ»Ñ, ÑĞºÑ– Ğ·Ğ³Ğ°Ğ´ÑƒÑ” ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡:

{json.dumps(smap, indent=2)}

Ğ¢ĞµĞºÑÑ‚:
"{instruction}"

ĞŸĞ¾Ğ²ĞµÑ€Ğ½Ğ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº "field:value", Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ñƒ.
"""

    try:
        resp = model.generate_content(prompt, generation_config={"temperature": 0})
        txt = resp.text.strip()
        if txt == "NONE":
            return []
        out = []
        for part in txt.split(","):
            if ":" in part:
                f, v = part.strip().split(":", 1)
                out.append((f, v))
        return out
    except Exception:
        return []


def find_matches_with_ai(instruction, smap):
    return find_matches_with_ai_cached(instruction, json.dumps(smap, sort_keys=True))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SPLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_into_separate_queries(message: str) -> list:
    try:
        prompt = f"""
Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ğ¾ĞºÑ€ĞµĞ¼Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸:

"{message}"

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚:
Ğ—ĞĞŸĞ˜Ğ¢_1: ...
Ğ—ĞĞŸĞ˜Ğ¢_2: ...
"""
        resp = model.generate_content(prompt, generation_config={"temperature": 0})
        lines = resp.text.strip().split("\n")
        out = []
        for ln in lines:
            if ln.startswith("Ğ—ĞĞŸĞ˜Ğ¢_"):
                q = ln.split(":", 1)[1].strip()
                out.append(q)
        return out if out else [message]
    except Exception:
        return [message]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SQL GENERATOR + METRIC PARSER INTEGRATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_sql(instruction_part: str, smap) -> str:
    """
    Ğ¢ÑƒÑ‚ Ğ¼Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ”Ğ¼Ğ¾ metric_parser.detect_metric + metric_loader.get_metrics
    Ñ– Ğ´Ğ°Ñ”Ğ¼Ğ¾ SQL-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ— Ğ¿Ñ–Ğ´ĞºĞ°Ğ·ĞºÑƒ Ğ· Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ñ.
    """

    # 1. Ğ”ĞµÑ‚ĞµĞºÑ†Ñ–Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    metric = detect_metric(instruction_part)
    metrics = get_metrics()

    metric_hint = f"\nĞ’Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°: {metric}\n" if metric else ""

    rev_schema, cost_schema = get_all_schemas()
    date_cols = _collect_date_columns(rev_schema) | _collect_date_columns(cost_schema)

    rev_cols = ", ".join([c["name"] for c in rev_schema]) if rev_schema else "(Ğ½ĞµĞ¼Ğ°Ñ” ÑÑ…ĞµĞ¼Ğ¸ REVENUE)"
    cost_cols = ", ".join([c["name"] for c in cost_schema]) if cost_schema else "(Ğ½ĞµĞ¼Ğ°Ñ” ÑÑ…ĞµĞ¼Ğ¸ COST)"

    sql_prompt = f"""
Ğ—Ğ³ĞµĞ½ĞµÑ€ÑƒĞ¹ BigQuery SQL Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½Ğ½Ñ:

"{instruction_part}"

{metric_hint}

ĞŸĞ¾Ğ²Ğ½Ñ– Ğ½Ğ°Ğ·Ğ²Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ÑŒ:
REVENUE_TABLE = `{REVENUE_TABLE_REF}`
COST_TABLE    = `{COST_TABLE_REF}`

Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ– Ğ¿Ğ¾Ğ»Ñ (Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸):
{metrics}

Ğ¡Ñ‚Ğ¾Ğ²Ğ¿Ñ†Ñ– Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– REVENUE (Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ– Ğ½Ğ°Ğ·Ğ²Ğ¸ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº):
{rev_cols}

Ğ¡Ñ‚Ğ¾Ğ²Ğ¿Ñ†Ñ– Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ– COST (Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ– Ğ½Ğ°Ğ·Ğ²Ğ¸ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº):
{cost_cols}

Ğ¡Ñ…ĞµĞ¼Ğ° REVENUE:
{json.dumps(rev_schema, indent=2)}

Ğ¡Ñ…ĞµĞ¼Ğ° COST:
{json.dumps(cost_schema, indent=2)}

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
- Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ñ‚Ñ– Ğ¿Ğ¾Ğ»Ñ, ÑĞºÑ– Ñ” Ğ² ÑĞ¿Ğ¸ÑĞºĞ°Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ²Ğ¸Ñ‰Ğµ. ĞĞµ Ğ²Ğ¸Ğ³Ğ°Ğ´ÑƒĞ¹ Ğ½Ğ¾Ğ²Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ñ–Ğ² (Ğ½Ğ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, event_type), ÑĞºÑ‰Ğ¾ Ñ—Ñ… Ğ½ĞµĞ¼Ğ°Ñ” Ğ² ÑÑ…ĞµĞ¼Ñ–.
- Ğ¯ĞºÑ‰Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ "opex", "cost", "Ğ²Ğ¸Ñ‚Ñ€Ğ°Ñ‚Ğ¸", "ÑĞ¿ĞµĞ½Ğ´" â€” Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ `{COST_TABLE_REF}`.
- Ğ¯ĞºÑ‰Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ revenue, Ğ´Ğ¾Ñ…Ñ–Ğ´, GMV â€” Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ `{REVENUE_TABLE_REF}`.
- Ğ”Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ñ–Ğ² (SUM, AVG, COUNT, Ñ‚Ğ¾Ñ‰Ğ¾) Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ ÑÑ‚Ğ°Ğ² alias, Ğ½Ğ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´: SELECT SUM(revenue) AS value.
- ĞĞµ Ğ·Ğ°Ğ»Ğ¸ÑˆĞ°Ğ¹ SELECT SUM(...) Ğ±ĞµĞ· alias, Ñ‰Ğ¾Ğ± Ğ½Ğ°Ğ·Ğ²Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ½Ğµ Ğ±ÑƒĞ»Ğ° f0_.
- Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ BigQuery SQL.
- ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ STRFTIME.
- Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ¹ CURRENT_DATE('{LOCAL_TZ}').
- ĞĞµ Ğ¿Ğ¸ÑˆĞ¸ ORDER BY Ñƒ window Ñ„ÑƒĞ½ĞºÑ†Ñ–ÑÑ…, ĞºÑ€Ñ–Ğ¼ Ğ²Ğ¸Ğ¿Ğ°Ğ´ĞºÑ–Ğ², ĞºĞ¾Ğ»Ğ¸ Ñ†Ğµ LAG/LEAD (BigQuery Ğ²Ğ¸Ğ¼Ğ°Ğ³Ğ°Ñ” ORDER BY Ğ´Ğ»Ñ Ñ†Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ñ–Ğ¹).
- ĞŸĞ¾Ğ²ĞµÑ€Ğ½Ğ¸ Ğ»Ğ¸ÑˆĞµ SQL Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑĞ½ĞµĞ½ÑŒ Ñ– Ğ±ĞµĞ· Markdown.
"""

    resp = model.generate_content(sql_prompt, generation_config={"temperature": 0})
    sql = resp.text.strip()
    sql = sql.replace("```sql", "").replace("```", "").strip()

    sql = fix_window_order_by(sql)
    sql = _sanitize_sql_dates(sql, date_cols)

    return sql


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXECUTE SINGLE QUERY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_single_query(instruction: str, smap: dict, user_id: str = "unknown") -> str:
    instruction_part = instruction.strip()
    if not instruction_part:
        return "ĞŸĞ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ”."

    matched = find_matches_with_ai(instruction_part, smap)
    for field, value in matched:
        instruction_part += f" ({field}='{value}')"

    sql_query = generate_sql(instruction_part, smap)

    try:
        df = execute_cached_query(sql_query)
    except Exception as e:
        msg = str(e)
        if RETURN_SQL_ON_ERROR:
            return f"âŒ SQL ERROR:\n```sql\n{sql_query}\n```\n{msg}"
        return f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ– SQL:\n{msg}"

    if df.empty:
        return "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹."

    # Ğ¯ĞºÑ‰Ğ¾ BigQuery Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ² Ğ¾Ğ´Ğ½Ñƒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ğ· Ñ–Ğ¼'ÑĞ¼ f0_, Ğ¿ĞµÑ€ĞµĞ¹Ğ¼ĞµĞ½ÑƒÑ”Ğ¼Ğ¾ Ñ—Ñ— Ğ² 'value',
    # Ñ‰Ğ¾Ğ± Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ²Ğ¸Ğ³Ğ»ÑĞ´Ğ°Ğ² Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾.
    if len(df.columns) == 1 and str(df.columns[0]).startswith("f0_"):
        df = df.rename(columns={df.columns[0]: "value"})

    # FINISH â†’ Vertex analysis
    analysis_prompt = f"""
ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ CSV Ğ½Ğ¸Ğ¶Ñ‡Ğµ:

{df.to_csv(index=False)}

Ğ†Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ñ–Ñ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡Ğ°:
"{instruction_part}"

Ğ—Ñ€Ğ¾Ğ±Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº (3â€“4 Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ).
"""

    resp = model.generate_content(analysis_prompt, generation_config={"temperature": 0})
    return resp.text.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN ENTRY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_slack_message(message: str, smap: dict, user_id: str = "unknown") -> str:
    queries = split_into_separate_queries(message)

    if len(queries) == 1:
        return execute_single_query(queries[0], smap, user_id)

    out = f"ğŸ“ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(queries)} Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ²:\n\n"
    for i, q in enumerate(queries, 1):
        ans = execute_single_query(q, smap, user_id)
        out += f"**Ğ—Ğ°Ğ¿Ğ¸Ñ‚ {i}:** {q}\n{ans}\n\n"
    return out


def run_analysis(message: str, semantic_map_override=None, user_id="unknown"):
    smap = semantic_map_override or semantic_map
    return process_slack_message(message, smap, user_id)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FIX WINDOW ORDER BY ERRORS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_window_order_by(sql: str) -> str:
    """
    ĞĞ±Ñ€Ğ¾Ğ±ĞºĞ° window-Ñ„ÑƒĞ½ĞºÑ†Ñ–Ğ¹:
    - Ğ¯ĞºÑ‰Ğ¾ Ğ²ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ñ– OVER(...) Ñ” ORDER BY Ñ– Ğ½ĞµĞ¼Ğ°Ñ” LAG/LEAD â†’ Ğ²Ğ¸Ğ´Ğ°Ğ»ÑÑ”Ğ¼Ğ¾ ORDER BY
    - Ğ¯ĞºÑ‰Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ LAG/LEAD â†’ Ğ·Ğ°Ğ»Ğ¸ÑˆĞ°Ñ”Ğ¼Ğ¾ ORDER BY (BigQuery Ğ¹Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ¼Ğ°Ğ³Ğ°Ñ”)
    Ğ—Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¸Ğ¹ ORDER BY Ñƒ ĞºÑ–Ğ½Ñ†Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ Ğ½Ğµ Ñ‡Ñ–Ğ¿Ğ°Ñ”Ğ¼Ğ¾.
    """

    def _fix(match: re.Match) -> str:
        over_clause = match.group(0)

        # Ğ¯ĞºÑ‰Ğ¾ Ñƒ Ğ²Ñ–ĞºĞ½Ñ– Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ LAG/LEAD â€” Ğ½Ğµ Ñ‡Ñ–Ğ¿Ğ°Ñ”Ğ¼Ğ¾
        if re.search(r"\bLAG\s*$begin:math:text$\|\\bLEAD\\s\*\\\(\"\, over\_clause\, flags\=re\.IGNORECASE\)\:
            return over\_clause

        \# ĞŸÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ ORDER BY ÑƒÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ñ– OVER\(\.\.\.\)
        cleaned \= re\.sub\(r\"ORDER\\s\+BY\[\^\)\]\*\"\, \"\"\, over\_clause\, flags\=re\.IGNORECASE\)
        return cleaned

    \# Ğ—Ğ°ÑÑ‚Ğ¾ÑÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ´Ğ¾ Ğ²ÑÑ–Ñ… OVER\(\.\.\.\)
    return re\.sub\(r\"OVER\\s\*\\\(\[\^\)\]\*$end:math:text$", _fix, sql, flags=re.IGNORECASE | re.DOTALL)
